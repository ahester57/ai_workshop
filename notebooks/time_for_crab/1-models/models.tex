\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{models}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \section{Model Selection}\label{model-selection}

\subparagraph{\texorpdfstring{\emph{In which we choose the best model to
predict the age of a
crab.}}{In which we choose the best model to predict the age of a crab.}}\label{in-which-we-choose-the-best-model-to-predict-the-age-of-a-crab.}

\href{https://github.com/ahester57/ai_workshop/tree/master/notebooks/time_for_crab/1-models}{GitHub
Repository}

\href{https://nbviewer.jupyter.org/github/ahester57/ai_workshop/blob/master/notebooks/time_for_crab/1-models/models.ipynb}{Notebook
Viewer}

\href{https://www.kaggle.com/sidhus/crab-age-prediction}{Kaggle Dataset}

    \subsection{Table of Contents}\label{table-of-contents}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \hyperref[define-constants]{Define Constants}
\item
  \hyperref[import-libraries]{Import Libraries}
\item
  \hyperref[load-data-from-cache]{Load Data from Cache}
\item
  \hyperref[split-the-data]{Split the Data}
\item
  \hyperref[metrics-used]{Metrics Used}
\item
  \hyperref[model-exploration]{Model Exploration}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \hyperref[naive-linear-regression]{Naive Linear Regression}
  \item
    \hyperref[neural-network-model]{Neural Network Model}
  \item
    \hyperref[neural-network-model-32-16-8-1]{Neural Network Model (32-16-8-1)}
  \item
    \hyperref[neural-network-model-16-8-1]{Neural Network Model (16-8-1)}
  \item
    \hyperref[neural-network-model-8-1]{Neural Network Model (8-1)}
  \item
    \hyperref[neural-network-model-4-1]{Neural Network Model (4-1)}
  \item
    \hyperref[neural-network-model-2-1]{Neural Network Model (2-1)}
  \item
    \hyperref[true-vs-predicted-age-scatter-plots]{True vs Predicted Age Scatter Plots}
  \item
    \hyperref[training-loss-over-time-plots]{Training Loss Over Time Plots}
  \item
    \hyperref[re-train-the-models-again]{Re-Train the Models Again}
  \item
    \hyperref[re-plot-the-training-loss-over-time]{Re-Plot the Training Loss Over Time}

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      \hyperref[training-loss-over-more-time-observations]{Training Loss Over More Time Observations}
    \end{enumerate}
  \end{enumerate}
\item
  \hyperref[model-leaderboard]{Model Leaderboard}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \hyperref[my-criteria]{My Criteria}
  \item
    \hyperref[putting-it-all-together]{Putting it All Together}
  \item
    \hyperref[reminder-of-our-metrics]{Reminder of Our Metrics}
  \item
    \hyperref[model-type-comparison]{Model Type Comparison}

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      \hyperref[score-comparison-observations]{Score Comparison Observations}
    \end{enumerate}
  \item
    \hyperref[show-the-leaderboard-again]{Show the Leaderboard Again}

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      \hyperref[score-these-scores]{Score These Scores}
    \end{enumerate}
  \end{enumerate}
\item
  \hyperref[choose-the-best-architecture-for-the-job]{Choose the Best Architecture for the Job}
\item
  \hyperref[hyperparameter-tuning]{Hyperparameter Tuning}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \hyperref[hyperparameters]{Hyperparameters}
  \item
    \hyperref[optimizer-tuning]{Optimizer Tuning}
  \item
    \hyperref[optimizer-decision]{Optimizer Decision}
  \item
    \hyperref[learning-rate-tuning]{Learning Rate Tuning}
  \item
    \hyperref[learning-rate-decision]{Learning Rate Decision}
  \item
    \hyperref[loss-function-to-mean-absolute-error]{Loss Function to Mean Absolute Error}
  \item
    \hyperref[perhaps-an-ensemble-will-help]{Perhaps an Ensemble Will Help}
  \end{enumerate}
\item
  \hyperref[winner-winner-crabs-for-dinner]{Winner, Winner, Crab’s for Dinner!}
\item
  \hyperref[onwards-to-feature-engineering]{Onwards to Feature Engineering}
\end{enumerate}

    \subsubsection{Define Constants}\label{define-constants}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{CACHE\PYZus{}FILE} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../cache/splitcrabs.feather}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{NEXT\PYZus{}NOTEBOOK} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../2\PYZhy{}features/features.ipynb}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../cache/best\PYZus{}model.weights.h5}\PY{l+s+s1}{\PYZsq{}}

\PY{n}{PREDICTION\PYZus{}TARGET} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}    \PY{c+c1}{\PYZsh{} \PYZsq{}Age\PYZsq{} is predicted}
\PY{n}{DATASET\PYZus{}COLUMNS} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sex\PYZus{}F}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sex\PYZus{}M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sex\PYZus{}I}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Diameter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Height}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shucked Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Viscera Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shell Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}
\PY{n}{REQUIRED\PYZus{}COLUMNS} \PY{o}{=} \PY{p}{[}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}

\PY{n}{NUM\PYZus{}EPOCHS} \PY{o}{=} \PY{l+m+mi}{100}
\PY{n}{VALIDATION\PYZus{}SPLIT} \PY{o}{=} \PY{l+m+mf}{0.2}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

    \subsubsection{Import Libraries}\label{import-libraries}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{k+kn}{from} \PY{n+nn}{notebooks}\PY{n+nn}{.}\PY{n+nn}{time\PYZus{}for\PYZus{}crab}\PY{n+nn}{.}\PY{n+nn}{mlutils} \PY{k+kn}{import} \PY{n}{display\PYZus{}df}\PY{p}{,} \PY{n}{generate\PYZus{}neural\PYZus{}pyramid}
\PY{k+kn}{from} \PY{n+nn}{notebooks}\PY{n+nn}{.}\PY{n+nn}{time\PYZus{}for\PYZus{}crab}\PY{n+nn}{.}\PY{n+nn}{mlutils} \PY{k+kn}{import} \PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{,} \PY{n}{plot\PYZus{}training\PYZus{}loss\PYZus{}from\PYZus{}dict}\PY{p}{,} \PY{n}{plot\PYZus{}true\PYZus{}vs\PYZus{}pred\PYZus{}from\PYZus{}dict}
\PY{k+kn}{from} \PY{n+nn}{notebooks}\PY{n+nn}{.}\PY{n+nn}{time\PYZus{}for\PYZus{}crab}\PY{n+nn}{.}\PY{n+nn}{mlutils} \PY{k+kn}{import} \PY{n}{score\PYZus{}combine}\PY{p}{,} \PY{n}{score\PYZus{}comparator}\PY{p}{,} \PY{n}{score\PYZus{}model}

\PY{k+kn}{import} \PY{n+nn}{keras}

\PY{n}{keras\PYZus{}backend} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{backend}\PY{o}{.}\PY{n}{backend}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Keras version: }\PY{l+s+si}{\PYZob{}}\PY{n}{keras}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Keras backend: }\PY{l+s+si}{\PYZob{}}\PY{n}{keras\PYZus{}backend}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{if} \PY{n}{keras\PYZus{}backend} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tensorflow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TensorFlow version: }\PY{l+s+si}{\PYZob{}}\PY{n}{tf}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TensorFlow devices: }\PY{l+s+si}{\PYZob{}}\PY{n}{tf}\PY{o}{.}\PY{n}{config}\PY{o}{.}\PY{n}{list\PYZus{}physical\PYZus{}devices}\PY{p}{(}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{elif} \PY{n}{keras\PYZus{}backend} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{torch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{k+kn}{import} \PY{n+nn}{torch}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Torch version: }\PY{l+s+si}{\PYZob{}}\PY{n}{torch}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Torch devices: }\PY{l+s+si}{\PYZob{}}\PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{get\PYZus{}device\PYZus{}name}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{current\PYZus{}device}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} torch supports windows\PYZhy{}native cuda, but CPU was faster for this task}
\PY{k}{elif} \PY{n}{keras\PYZus{}backend} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{k+kn}{import} \PY{n+nn}{jax}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{JAX version: }\PY{l+s+si}{\PYZob{}}\PY{n}{jax}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{JAX devices: }\PY{l+s+si}{\PYZob{}}\PY{n}{jax}\PY{o}{.}\PY{n}{devices}\PY{p}{(}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unknown backend; Proceed with caution.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}

\PY{k+kn}{from} \PY{n+nn}{typing} \PY{k+kn}{import} \PY{n}{Generator}

\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{o}{\PYZpc{}}\PY{n}{matplotlib} \PY{n}{inline}
\PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}

\PY{n}{pd}\PY{o}{.}\PY{n}{set\PYZus{}option}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mode.copy\PYZus{}on\PYZus{}write}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Keras version: 3.3.3
Keras backend: tensorflow
TensorFlow version: 2.16.1
TensorFlow devices: [PhysicalDevice(name='/physical\_device:CPU:0',
device\_type='CPU')]
CPU times: total: 375 ms
Wall time: 2.68 s
    \end{Verbatim}

    \subsubsection{Load Data from Cache}\label{load-data-from-cache}

In the \href{../0-eda/eda.ipynb}{exploratory data analysis section}, we
saved the cleaned and split data to a cache file. Let's load it back.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{crabs} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}feather}\PY{p}{(}\PY{n}{CACHE\PYZus{}FILE}\PY{p}{)}
\PY{n}{crabs\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}feather}\PY{p}{(}\PY{n}{CACHE\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.feather}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}test.feather}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{display\PYZus{}df}\PY{p}{(}\PY{n}{crabs}\PY{p}{,} \PY{n}{show\PYZus{}distinct}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} split features from target}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{crabs}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{crabs}\PY{p}{[}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}

\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{crabs\PYZus{}test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{crabs\PYZus{}test}\PY{p}{[}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}train: }\PY{l+s+si}{\PYZob{}}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}test: }\PY{l+s+si}{\PYZob{}}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
DataFrame shape: (3031, 11)
First 5 rows:
        Length  Diameter    Height    Weight  Shucked Weight  Viscera Weight  \textbackslash{}
3483  1.724609  1.312500  0.500000  50.53125       25.984375        9.429688
993   1.612305  1.312500  0.500000  41.09375       17.031250        7.273438
1427  1.650391  1.262695  0.475098  40.78125       19.203125        8.078125
3829  1.362305  1.150391  0.399902  25.43750        9.664062        4.691406
1468  1.250000  0.924805  0.375000  30.09375       14.007812        6.320312

      Shell Weight  Sex\_F  Sex\_I  Sex\_M  Age
3483     13.070312  False  False   True   12
993      14.320312   True  False  False   13
1427      5.046875  False  False   True   11
3829      9.781250  False  False   True   10
1468      8.390625  False  False   True    9
<class 'pandas.core.frame.DataFrame'>
Index: 3031 entries, 3483 to 658
Data columns (total 11 columns):
 \#   Column          Non-Null Count  Dtype
---  ------          --------------  -----
 0   Length          3031 non-null   float16
 1   Diameter        3031 non-null   float16
 2   Height          3031 non-null   float16
 3   Weight          3031 non-null   float16
 4   Shucked Weight  3031 non-null   float16
 5   Viscera Weight  3031 non-null   float16
 6   Shell Weight    3031 non-null   float16
 7   Sex\_F           3031 non-null   bool
 8   Sex\_I           3031 non-null   bool
 9   Sex\_M           3031 non-null   bool
 10  Age             3031 non-null   int8
dtypes: bool(3), float16(7), int8(1)
memory usage: 77.0 KB
Info:
None
Length distinct values:
[1.725  1.612  1.65   1.362  1.25   1.6875 1.487  1.5625 1.4375 1.45  ]
Diameter distinct values:
[1.3125 1.263  1.15   0.925  1.2    1.162  0.8877 0.8374 1.388  1.0625]
Height distinct values:
[0.5    0.475  0.4    0.375  0.4624 0.425  0.4126 0.4375 0.2876 0.2625]
Weight distinct values:
[50.53 41.1  40.78 25.44 30.1  45.   32.03 32.38 30.19 29.34]
Shucked Weight distinct values:
[25.98  17.03  19.2    9.664 14.01  19.66  16.16  16.42  14.13  11.37 ]
Viscera Weight distinct values:
[9.43  7.273 8.08  4.69  6.32  9.52  7.242 6.082 5.29  2.623]
Shell Weight distinct values:
[13.07  14.32   5.047  9.78   8.39  11.195  7.51   8.22   7.98  10.914]
Sex\_F distinct values:
[False  True]
Sex\_I distinct values:
[False  True]
Sex\_M distinct values:
[ True False]
Age distinct values:
[12 13 11 10  9  8 17  6 19  7]
X\_train: (3031, 10)
X\_test: (759, 10)
CPU times: total: 0 ns
Wall time: 16 ms
    \end{Verbatim}

    \subsection{Metrics Used}\label{metrics-used}

Throughout this notebook, we will use the following metrics to evaluate
the regression model:

\paragraph{Mean Squared Error}\label{mean-squared-error}

\begin{itemize}
\tightlist
\item
  The best score is 0.0
\item
  Lower is better.
\item
  Larger errors are penalized more than smaller errors.
\end{itemize}

\paragraph{Mean Absolute Error}\label{mean-absolute-error}

\begin{itemize}
\tightlist
\item
  The best score is 0.0
\item
  Lower is better.
\item
  Less sensitive to outliers.
\end{itemize}

\paragraph{Explained Variance Score}\label{explained-variance-score}

\begin{itemize}
\tightlist
\item
  The best score is 1.0
\item
  Lower is worse.
\end{itemize}

\paragraph{R2 Score}\label{r2-score}

\begin{itemize}
\tightlist
\item
  The best score is 1.0
\item
  Lower is worse.
\end{itemize}

From the
\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html}{scikit-learn
documentation}:

\begin{quote}
\textbf{Note}: The Explained Variance score is similar to the
\texttt{R\^{}2\ score}, with the notable difference that it does not
account for systematic offsets in the prediction. Most often the
\texttt{R\^{}2\ score} should be preferred.
\end{quote}

    \subsection{Model Exploration}\label{model-exploration}

So far, we have not done any feature engineering, which can often be the
most important part of the process. Some new features could be
constructed from our dataset which would call for a different model.
Nonetheless, we can start by using all features to set a baseline.

We will start with a few simple models to get a baseline accuracy.

We will use the following models: - Naive Random Baseline - Linear
Regression - Neural Networks - (64-32-16-8-1) - (32-16-8-1) - (16-8-1) -
(8-1) - (4-1) - (2-1)

\subsubsection{Naive Linear Regression}\label{naive-linear-regression}

The simplest model is a naive linear regression model. It is untrained
and will make random guesses.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} layer: input}
\PY{n}{layer\PYZus{}feature\PYZus{}input} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{,}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} layer: normalizer}
\PY{n}{layer\PYZus{}feature\PYZus{}normalizer} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Normalization}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{layer\PYZus{}feature\PYZus{}normalizer}\PY{o}{.}\PY{n}{adapt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} layer: output (linear regression)}
\PY{n}{layer\PYZus{}feature\PYZus{}output} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} architecture:}
\PY{c+c1}{\PYZsh{}   input \PYZhy{}\PYZgt{} normalizer \PYZhy{}\PYZgt{} linear}
\PY{c+c1}{\PYZsh{} initialize the all\PYZus{}models dictionary}
\PY{n}{all\PYZus{}models} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
    \PY{n}{layer\PYZus{}feature\PYZus{}input}\PY{p}{,}
    \PY{n}{layer\PYZus{}feature\PYZus{}normalizer}\PY{p}{,}
    \PY{n}{layer\PYZus{}feature\PYZus{}output}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{11} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{32} (132.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{11} (44.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 15.6 ms
Wall time: 50.1 ms
    \end{Verbatim}

    \paragraph{Configure the Linear Model}\label{configure-the-linear-model}

\emph{These will be used for all models unless otherwise specified.}

\begin{itemize}
\tightlist
\item
  \textbf{Optimizer}

  \begin{itemize}
  \tightlist
  \item
    Adam: Adaptive Moment Estimation
    \href{https://arxiv.org/abs/1412.6980}{(Kingma \& Ba, 2014)}
  \end{itemize}
\item
  \textbf{Loss Function}

  \begin{itemize}
  \tightlist
  \item
    Mean Squared Error (MSE)

    \begin{itemize}
    \tightlist
    \item
      This penalizes larger errors more than smaller errors.
    \item
      We took out outliers in the data cleaning step, so this should
      perform better.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Callbacks}

  \begin{itemize}
  \tightlist
  \item
    Model Checkpoint

    \begin{itemize}
    \tightlist
    \item
      Save the best model weights.
    \end{itemize}
  \end{itemize}
\end{itemize}

\subparagraph{Define Common Compile
Options}\label{define-common-compile-options}

\subparagraph{Define Common Checkpoint
Options}\label{define-common-checkpoint-options}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} some framework}
\PY{k}{def} \PY{n+nf}{next\PYZus{}adam}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{p}{:}\PY{n+nb}{float}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{Generator}\PY{p}{[}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{]}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Yield the next Adam optimizer with the given learning rate.\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{yield} \PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{n}{learning\PYZus{}rate}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}
        \PY{n}{optimizer}\PY{p}{:}\PY{n}{keras}\PY{o}{.}\PY{n}{Optimizer}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,}
        \PY{n}{loss\PYZus{}metric}\PY{p}{:}\PY{n+nb}{str}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Return a dictionary of common compile options.}

\PY{l+s+sd}{    :param optimizer: The optimizer to use. Defaults to Adam with LR=0.001.}
\PY{l+s+sd}{    :param loss\PYZus{}metric: The loss metric to use. Defaults to \PYZsq{}mean\PYZus{}squared\PYZus{}error\PYZsq{}.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{return} \PY{p}{\PYZob{}}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{optimizer} \PY{k}{if} \PY{n}{optimizer} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{k}{else} \PY{n+nb}{next}\PY{p}{(}\PY{n}{next\PYZus{}adam}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{loss\PYZus{}metric}
    \PY{p}{\PYZcb{}}


\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{common\PYZus{}checkpoint\PYZus{}options} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{monitor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{save\PYZus{}best\PYZus{}only}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{k+kc}{True}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{save\PYZus{}weights\PYZus{}only}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{k+kc}{True}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min}\PY{l+s+s1}{\PYZsq{}}
\PY{p}{\PYZcb{}}

\PY{n}{linear\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}linear.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 6 ms
    \end{Verbatim}

    \paragraph{Score the Linear Model (Before
Training)}\label{score-the-linear-model-before-training}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{untrained\PYZus{}linear\PYZus{}preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Utility functions imported from mlutils.py}
\PY{n}{untrained\PYZus{}linear\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{untrained\PYZus{}linear\PYZus{}preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{untrained\PYZus{}linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{untrained\PYZus{}linear\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 958us/step
CPU times: total: 31.2 ms
Wall time: 103 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear          101.943787             9.748023

                  explained\_variance\_score   r2\_score
untrained\_linear                  0.049686 -13.000124
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Train the Linear Model}\label{train-the-linear-model}

\subparagraph{Define Common Fit
Options}\label{define-common-fit-options}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{common\PYZus{}fit\PYZus{}options} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{X\PYZus{}train}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{y\PYZus{}train}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{NUM\PYZus{}EPOCHS}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{verbose}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{VALIDATION\PYZus{}SPLIT}
\PY{p}{\PYZcb{}}

\PY{n}{linear\PYZus{}history} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
    \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{linear\PYZus{}checkpoint}\PY{p}{]}
\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}linear.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 1.98 s
Wall time: 7.72 s
    \end{Verbatim}

    \paragraph{Score the Linear Model}\label{score-the-linear-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{linear\PYZus{}preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{linear\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{linear\PYZus{}preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{linear\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 438us/step
CPU times: total: 31.2 ms
Wall time: 50.6 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear          101.943787             9.748023
linear                     13.669806             3.097228

                  explained\_variance\_score   r2\_score
untrained\_linear                  0.049686 -13.000124
linear                           -0.187996  -2.867148
\end{Verbatim}
\end{tcolorbox}
        
    \subsubsection{Neural Network Model}\label{neural-network-model}

\paragraph{Neural Network
Architecture}\label{neural-network-architecture}

We will start with a deep (64-32-16-8-1) neural network with a few
layers, gradually reducing the complexity from our overfit model.

\begin{itemize}
\tightlist
\item
  \textbf{Input Layer}

  \begin{itemize}
  \tightlist
  \item
    All of the features, please.
  \end{itemize}
\item
  \textbf{Normalizer Layer}

  \begin{itemize}
  \tightlist
  \item
    Adapted to all features in the training data.
  \end{itemize}
\item
  \textbf{Hidden Layers}

  \begin{itemize}
  \tightlist
  \item
    Four dense layers each with 64 \textgreater\textgreater{}
    \{layer\_index\} units and ReLU activation.
  \end{itemize}
\item
  \textbf{Output Layer}

  \begin{itemize}
  \tightlist
  \item
    Layer with one output.
  \end{itemize}
\end{itemize}

\begin{quote}
\emph{I know what you're thinking}: ``Why not start with a simpler
model?''

\emph{My answer to that}: This is for science, and we're going to test
them all anyway. It's sometimes easier to copy and delete than it is to
build from scratch.
\end{quote}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} layer: input \PYZhy{} reused from linear model}
\PY{c+c1}{\PYZsh{} layer: normalizer \PYZhy{} reused from linear model}

\PY{c+c1}{\PYZsh{} layer(s): hidden (relu) \PYZhy{} 64, 32, 16, 8}
\PY{n}{num\PYZus{}hidden\PYZus{}layers} \PY{o}{=} \PY{l+m+mi}{4}
\PY{n}{num\PYZus{}units} \PY{o}{=} \PY{l+m+mi}{64}
\PY{n}{layer\PYZus{}deepest\PYZus{}hidden\PYZus{}relu\PYZus{}list} \PY{o}{=} \PY{n}{generate\PYZus{}neural\PYZus{}pyramid}\PY{p}{(}\PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{p}{,} \PY{n}{num\PYZus{}units}\PY{p}{)}

\PY{c+c1}{\PYZsh{} layer: output (linear regression)}
\PY{n}{layer\PYZus{}deepest\PYZus{}output} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} architecture:}
\PY{c+c1}{\PYZsh{}   input \PYZhy{}\PYZgt{} normalizer \PYZhy{}\PYZgt{} hidden(s) \PYZhy{}\PYZgt{} dense}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
    \PY{n}{layer\PYZus{}feature\PYZus{}input}\PY{p}{,}
    \PY{n}{layer\PYZus{}feature\PYZus{}normalizer}\PY{p}{,}
    \PY{o}{*}\PY{n}{layer\PYZus{}deepest\PYZus{}hidden\PYZus{}relu\PYZus{}list}\PY{p}{,}
    \PY{n}{layer\PYZus{}deepest\PYZus{}output}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_1"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_1 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{64})             │           \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{704} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_2 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{32})             │         \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{2,080} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_3 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{16})             │           \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{528} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_4 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │           \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{136} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_5 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{3,478} (13.59 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{3,457} (13.50 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 15.6 ms
Wall time: 26 ms
    \end{Verbatim}

    \paragraph{Configure the Neural Network
Model}\label{configure-the-neural-network-model}

\begin{itemize}
\tightlist
\item
  \textbf{Optimizer}

  \begin{itemize}
  \tightlist
  \item
    Adam: Adaptive Moment Estimation
    \href{https://arxiv.org/abs/1412.6980}{(Kingma \& Ba, 2014)}
  \end{itemize}
\item
  \textbf{Loss Function}

  \begin{itemize}
  \tightlist
  \item
    Mean Squared Error (MSE)

    \begin{itemize}
    \tightlist
    \item
      This penalizes larger errors more than smaller errors.
    \item
      We took out outliers in the data cleaning step, so this should
      perform better.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Callbacks}

  \begin{itemize}
  \tightlist
  \item
    Model Checkpoint
  \end{itemize}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{deepest\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 1 ms
    \end{Verbatim}

    \paragraph{Train the Neural Network
Model}\label{train-the-neural-network-model}

\emph{We're not going to predict with the untrained model, as we already
have a random baseline on the leaderboard.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deepest\PYZus{}history} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
    \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deepest\PYZus{}checkpoint}\PY{p}{]}
\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 2.41 s
Wall time: 9.09 s
    \end{Verbatim}

    \paragraph{Score the Neural Network
Model}\label{score-the-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deepest\PYZus{}preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{deepest\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{deepest\PYZus{}preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deepest\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 2ms/step
CPU times: total: 93.8 ms
Wall time: 109 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear          101.943787             9.748023
linear                     13.669806             3.097228
64\_32\_16\_8\_1                3.746227             1.420128

                  explained\_variance\_score   r2\_score
untrained\_linear                  0.049686 -13.000124
linear                           -0.187996  -2.867148
64\_32\_16\_8\_1                      0.202596   0.202460
\end{Verbatim}
\end{tcolorbox}
        
    \subsubsection{Neural Network Model
(32-16-8-1)}\label{neural-network-model-32-16-8-1}

Let's cut the first layer out and see if it still has what it takes.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} layer: input \PYZhy{} reused from linear model}
\PY{c+c1}{\PYZsh{} layer: normalizer \PYZhy{} reused from linear model}

\PY{c+c1}{\PYZsh{} layer(s): hidden (relu) \PYZhy{} 32, 16, 8}
\PY{n}{num\PYZus{}hidden\PYZus{}layers} \PY{o}{=} \PY{l+m+mi}{3}
\PY{n}{num\PYZus{}units} \PY{o}{=} \PY{l+m+mi}{32}
\PY{n}{layer\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}hidden\PYZus{}relu\PYZus{}list} \PY{o}{=} \PY{n}{generate\PYZus{}neural\PYZus{}pyramid}\PY{p}{(}\PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{p}{,} \PY{n}{num\PYZus{}units}\PY{p}{)}

\PY{c+c1}{\PYZsh{} layer: output (linear regression)}
\PY{n}{layer\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}output} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} architecture:}
\PY{c+c1}{\PYZsh{}   input \PYZhy{}\PYZgt{} normalizer \PYZhy{}\PYZgt{} hidden(s) \PYZhy{}\PYZgt{} dense}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
    \PY{n}{layer\PYZus{}feature\PYZus{}input}\PY{p}{,}
    \PY{n}{layer\PYZus{}feature\PYZus{}normalizer}\PY{p}{,}
    \PY{o}{*}\PY{n}{layer\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}hidden\PYZus{}relu\PYZus{}list}\PY{p}{,}
    \PY{n}{layer\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}output}
\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_2"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_6 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{32})             │           \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{352} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_7 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{16})             │           \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{528} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_8 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │           \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{136} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_9 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                 │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,046} (4.09 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1,025} (4.00 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 19 ms
    \end{Verbatim}

    \paragraph{Configure the (32-16-8-1) Neural Network
Model}\label{configure-the-32-16-8-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{deep\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 2 ms
    \end{Verbatim}

    \paragraph{Train the (32-16-8-1) Neural Network
Model}\label{train-the-32-16-8-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deep\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}history} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
    \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}checkpoint}\PY{p}{]}
\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 2.77 s
Wall time: 8.75 s
    \end{Verbatim}

    \paragraph{Score the (32-16-8-1) Neural Network
Model}\label{score-the-32-16-8-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deep\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{deep\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{deep\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 2ms/step
CPU times: total: 46.9 ms
Wall time: 103 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear          101.943787             9.748023
linear                     13.669806             3.097228
64\_32\_16\_8\_1                3.746227             1.420128
32\_16\_8\_1                   3.892990             1.436031

                  explained\_variance\_score   r2\_score
untrained\_linear                  0.049686 -13.000124
linear                           -0.187996  -2.867148
64\_32\_16\_8\_1                      0.202596   0.202460
32\_16\_8\_1                         0.174743   0.174401
\end{Verbatim}
\end{tcolorbox}
        
    \subsubsection{Neural Network Model
(16-8-1)}\label{neural-network-model-16-8-1}

The last one held up, so let's reduce it even more.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} layer: input \PYZhy{} reused from linear model}
\PY{c+c1}{\PYZsh{} layer: normalizer \PYZhy{} reused from linear model}

\PY{c+c1}{\PYZsh{} layer(s): hidden (relu) \PYZhy{} 16, 8}
\PY{n}{num\PYZus{}hidden\PYZus{}layers} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{num\PYZus{}units} \PY{o}{=} \PY{l+m+mi}{16}
\PY{n}{layer\PYZus{}16\PYZus{}8\PYZus{}hidden\PYZus{}relu\PYZus{}list} \PY{o}{=} \PY{n}{generate\PYZus{}neural\PYZus{}pyramid}\PY{p}{(}\PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{p}{,} \PY{n}{num\PYZus{}units}\PY{p}{)}

\PY{c+c1}{\PYZsh{} layer: output (linear regression)}
\PY{n}{layer\PYZus{}16\PYZus{}8\PYZus{}output} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} architecture:}
\PY{c+c1}{\PYZsh{}   input \PYZhy{}\PYZgt{} normalizer \PYZhy{}\PYZgt{} hidden(s) \PYZhy{}\PYZgt{} dense}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
    \PY{n}{layer\PYZus{}feature\PYZus{}input}\PY{p}{,}
    \PY{n}{layer\PYZus{}feature\PYZus{}normalizer}\PY{p}{,}
    \PY{o}{*}\PY{n}{layer\PYZus{}16\PYZus{}8\PYZus{}hidden\PYZus{}relu\PYZus{}list}\PY{p}{,}
    \PY{n}{layer\PYZus{}16\PYZus{}8\PYZus{}output}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_3"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_10 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{16})             │           \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{176} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_11 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │           \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{136} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_12 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{342} (1.34 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{321} (1.25 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 19 ms
    \end{Verbatim}

    \paragraph{Configure the (16-8-1) Neural Network
Model}\label{configure-the-16-8-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{deep\PYZus{}16\PYZus{}8\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}16\PYZus{}8.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 2.51 ms
    \end{Verbatim}

    \paragraph{Train the (16-8-1) Neural Network
Model}\label{train-the-16-8-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deep\PYZus{}16\PYZus{}8\PYZus{}history} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
    \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}16\PYZus{}8\PYZus{}checkpoint}\PY{p}{]}
\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}16\PYZus{}8.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 3.41 s
Wall time: 8.32 s
    \end{Verbatim}

    \paragraph{Score the (16-8-1) Neural Network
Model}\label{score-the-16-8-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deep\PYZus{}16\PYZus{}8\PYZus{}preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{deep\PYZus{}16\PYZus{}8\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{deep\PYZus{}16\PYZus{}8\PYZus{}preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}16\PYZus{}8\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 62.5 ms
Wall time: 87.8 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear          101.943787             9.748023
linear                     13.669806             3.097228
64\_32\_16\_8\_1                3.746227             1.420128
32\_16\_8\_1                   3.892990             1.436031
16\_8\_1                      3.791295             1.422898

                  explained\_variance\_score   r2\_score
untrained\_linear                  0.049686 -13.000124
linear                           -0.187996  -2.867148
64\_32\_16\_8\_1                      0.202596   0.202460
32\_16\_8\_1                         0.174743   0.174401
16\_8\_1                            0.166290   0.165454
\end{Verbatim}
\end{tcolorbox}
        
    \subsubsection{Neural Network Model
(8-1)}\label{neural-network-model-8-1}

The last reduction didn't lose too much accuracy, so let's continue
removing layers.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} layer: input \PYZhy{} reused from linear model}
\PY{c+c1}{\PYZsh{} layer: normalizer \PYZhy{} reused from linear model}

\PY{c+c1}{\PYZsh{} layer(s): hidden (relu) \PYZhy{} 8}
\PY{n}{num\PYZus{}hidden\PYZus{}layers} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{num\PYZus{}units} \PY{o}{=} \PY{l+m+mi}{8}
\PY{n}{layer\PYZus{}8\PYZus{}hidden\PYZus{}relu\PYZus{}list} \PY{o}{=} \PY{n}{generate\PYZus{}neural\PYZus{}pyramid}\PY{p}{(}\PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{p}{,} \PY{n}{num\PYZus{}units}\PY{p}{)}

\PY{c+c1}{\PYZsh{} layer: output (linear regression)}
\PY{n}{layer\PYZus{}8\PYZus{}output} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} architecture:}
\PY{c+c1}{\PYZsh{}   input \PYZhy{}\PYZgt{} normalizer \PYZhy{}\PYZgt{} hidden(s) \PYZhy{}\PYZgt{} dense}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
    \PY{n}{layer\PYZus{}feature\PYZus{}input}\PY{p}{,}
    \PY{n}{layer\PYZus{}feature\PYZus{}normalizer}\PY{p}{,}
    \PY{o}{*}\PY{n}{layer\PYZus{}8\PYZus{}hidden\PYZus{}relu\PYZus{}list}\PY{p}{,}
    \PY{n}{layer\PYZus{}8\PYZus{}output}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{118} (476.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 12.5 ms
    \end{Verbatim}

    \paragraph{Configure the (8-1) Neural Network
Model}\label{configure-the-8-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 2 ms
    \end{Verbatim}

    \paragraph{Train the (8-1) Neural Network
Model}\label{train-the-8-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deep\PYZus{}8\PYZus{}history} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
    \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}checkpoint}\PY{p}{]}
\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 2.45 s
Wall time: 7.92 s
    \end{Verbatim}

    \paragraph{Score the (8-1) Neural Network
Model}\label{score-the-8-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deep\PYZus{}8\PYZus{}preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{deep\PYZus{}8\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{deep\PYZus{}8\PYZus{}preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}8\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 15.6 ms
Wall time: 79.5 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear          101.943787             9.748023
linear                     13.669806             3.097228
64\_32\_16\_8\_1                3.746227             1.420128
32\_16\_8\_1                   3.892990             1.436031
16\_8\_1                      3.791295             1.422898
8\_1                         3.994874             1.469987

                  explained\_variance\_score   r2\_score
untrained\_linear                  0.049686 -13.000124
linear                           -0.187996  -2.867148
64\_32\_16\_8\_1                      0.202596   0.202460
32\_16\_8\_1                         0.174743   0.174401
16\_8\_1                            0.166290   0.165454
8\_1                               0.151188   0.151183
\end{Verbatim}
\end{tcolorbox}
        
    \subsubsection{Neural Network Model
(4-1)}\label{neural-network-model-4-1}

Still not too shabby. Let's reduce the last hidden layer to 4 neurons.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} layer: input \PYZhy{} reused from linear model}
\PY{c+c1}{\PYZsh{} layer: normalizer \PYZhy{} reused from linear model}

\PY{c+c1}{\PYZsh{} layer(s): hidden (relu) \PYZhy{} 4}
\PY{n}{num\PYZus{}hidden\PYZus{}layers} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{num\PYZus{}units} \PY{o}{=} \PY{l+m+mi}{4}
\PY{n}{layer\PYZus{}4\PYZus{}hidden\PYZus{}relu\PYZus{}list} \PY{o}{=} \PY{n}{generate\PYZus{}neural\PYZus{}pyramid}\PY{p}{(}\PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{p}{,} \PY{n}{num\PYZus{}units}\PY{p}{)}

\PY{c+c1}{\PYZsh{} layer: output (linear regression)}
\PY{n}{layer\PYZus{}4\PYZus{}output} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} architecture:}
\PY{c+c1}{\PYZsh{}   input \PYZhy{}\PYZgt{} normalizer \PYZhy{}\PYZgt{} hidden(s) \PYZhy{}\PYZgt{} dense}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
    \PY{n}{layer\PYZus{}feature\PYZus{}input}\PY{p}{,}
    \PY{n}{layer\PYZus{}feature\PYZus{}normalizer}\PY{p}{,}
    \PY{o}{*}\PY{n}{layer\PYZus{}4\PYZus{}hidden\PYZus{}relu\PYZus{}list}\PY{p}{,}
    \PY{n}{layer\PYZus{}4\PYZus{}output}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_5"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_15 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{4})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{44} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_16 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{5} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{70} (284.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{49} (196.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 14 ms
    \end{Verbatim}

    \paragraph{Configure the (4-1) Neural Network
Model}\label{configure-the-4-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}}\PY{k}{time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{deep\PYZus{}4\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}4.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

    \paragraph{Train the (4-1) Neural Network
Model}\label{train-the-4-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deep\PYZus{}4\PYZus{}history} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
    \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}4\PYZus{}checkpoint}\PY{p}{]}
\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}4.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 3.09 s
Wall time: 7.97 s
    \end{Verbatim}

    \paragraph{Score the (4-1) Neural Network
Model}\label{score-the-4-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deep\PYZus{}4\PYZus{}preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{deep\PYZus{}4\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{deep\PYZus{}4\PYZus{}preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}4\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 15.6 ms
Wall time: 82.3 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear          101.943787             9.748023
linear                     13.669806             3.097228
64\_32\_16\_8\_1                3.746227             1.420128
32\_16\_8\_1                   3.892990             1.436031
16\_8\_1                      3.791295             1.422898
8\_1                         3.994874             1.469987
4\_1                         7.346774             2.011863

                  explained\_variance\_score   r2\_score
untrained\_linear                  0.049686 -13.000124
linear                           -0.187996  -2.867148
64\_32\_16\_8\_1                      0.202596   0.202460
32\_16\_8\_1                         0.174743   0.174401
16\_8\_1                            0.166290   0.165454
8\_1                               0.151188   0.151183
4\_1                               0.148698   0.058166
\end{Verbatim}
\end{tcolorbox}
        
    \subsubsection{Neural Network Model
(2-1)}\label{neural-network-model-2-1}

The last reduction didn't lose too much accuracy, so let's continue
removing layers.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} layer: input \PYZhy{} reused from linear model}
\PY{c+c1}{\PYZsh{} layer: normalizer \PYZhy{} reused from linear model}

\PY{c+c1}{\PYZsh{} layer(s): hidden (relu) \PYZhy{} 2}
\PY{n}{num\PYZus{}hidden\PYZus{}layers} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{num\PYZus{}units} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{layer\PYZus{}2\PYZus{}hidden\PYZus{}relu\PYZus{}list} \PY{o}{=} \PY{n}{generate\PYZus{}neural\PYZus{}pyramid}\PY{p}{(}\PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{p}{,} \PY{n}{num\PYZus{}units}\PY{p}{)}

\PY{c+c1}{\PYZsh{} layer: output (linear regression)}
\PY{n}{layer\PYZus{}2\PYZus{}output} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} architecture:}
\PY{c+c1}{\PYZsh{}   input \PYZhy{}\PYZgt{} normalizer \PYZhy{}\PYZgt{} hidden(s) \PYZhy{}\PYZgt{} dense}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
    \PY{n}{layer\PYZus{}feature\PYZus{}input}\PY{p}{,}
    \PY{n}{layer\PYZus{}feature\PYZus{}normalizer}\PY{p}{,}
    \PY{o}{*}\PY{n}{layer\PYZus{}2\PYZus{}hidden\PYZus{}relu\PYZus{}list}\PY{p}{,}
    \PY{n}{layer\PYZus{}2\PYZus{}output}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_6"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_17 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{2})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{22} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_18 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{3} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{46} (188.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{25} (100.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 15 ms
    \end{Verbatim}

    \paragraph{Configure the (2-1) Neural Network
Model}\label{configure-the-2-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{deep\PYZus{}2\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}2.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 1 ms
    \end{Verbatim}

    \paragraph{Train the (2-1) Neural Network
Model}\label{train-the-2-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deep\PYZus{}2\PYZus{}history} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
    \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}2\PYZus{}checkpoint}\PY{p}{]}
\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}2.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 2.5 s
Wall time: 8 s
    \end{Verbatim}

    \paragraph{Score the (2-1) Neural Network
Model}\label{score-the-2-1-neural-network-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{deep\PYZus{}2\PYZus{}preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{deep\PYZus{}2\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{deep\PYZus{}2\PYZus{}preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}2\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 15.6 ms
Wall time: 81.2 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear          101.943787             9.748023
linear                     13.669806             3.097228
64\_32\_16\_8\_1                3.746227             1.420128
32\_16\_8\_1                   3.892990             1.436031
16\_8\_1                      3.791295             1.422898
8\_1                         3.994874             1.469987
4\_1                         7.346774             2.011863
2\_1                         7.742756             2.080535

                  explained\_variance\_score   r2\_score
untrained\_linear                  0.049686 -13.000124
linear                           -0.187996  -2.867148
64\_32\_16\_8\_1                      0.202596   0.202460
32\_16\_8\_1                         0.174743   0.174401
16\_8\_1                            0.166290   0.165454
8\_1                               0.151188   0.151183
4\_1                               0.148698   0.058166
2\_1                               0.144014   0.050772
\end{Verbatim}
\end{tcolorbox}
        
    We're finally showing signs of degredation at the (2-1) model. Let's see
how they all compare.

    \subsubsection{True vs Predicted Age Scatter
Plots}\label{true-vs-predicted-age-scatter-plots}

This gives us a good view of how well the model is predicting the age of
the crabs.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}preds} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{untrained\PYZus{}linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{true}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{untrained\PYZus{}linear\PYZus{}preds}\PY{p}{\PYZcb{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{true}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{linear\PYZus{}preds}\PY{p}{\PYZcb{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{true}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deepest\PYZus{}preds}\PY{p}{\PYZcb{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{true}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deep\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}preds}\PY{p}{\PYZcb{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{true}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deep\PYZus{}16\PYZus{}8\PYZus{}preds}\PY{p}{\PYZcb{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{true}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deep\PYZus{}8\PYZus{}preds}\PY{p}{\PYZcb{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{true}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deep\PYZus{}4\PYZus{}preds}\PY{p}{\PYZcb{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{true}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pred}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deep\PYZus{}2\PYZus{}preds}\PY{p}{\PYZcb{}}
\PY{p}{\PYZcb{}}

\PY{n}{plot\PYZus{}true\PYZus{}vs\PYZus{}pred\PYZus{}from\PYZus{}dict}\PY{p}{(}\PY{n}{all\PYZus{}preds}\PY{p}{,} \PY{n}{show\PYZus{}target\PYZus{}line}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 31.2 ms
Wall time: 53.1 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_69_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    

    \paragraph{True vs Predicted Age Scatter Plot
Observations}\label{true-vs-predicted-age-scatter-plot-observations}

Neat!

\emph{\textbf{Note}: The line of truth is shown in green.}

\subparagraph{Untrained Linear Model}\label{untrained-linear-model}

\begin{itemize}
\tightlist
\item
  Very bad.

  \begin{itemize}
  \tightlist
  \item
    As usual.
  \end{itemize}
\end{itemize}

\subparagraph{Linear Model}\label{linear-model}

\begin{itemize}
\tightlist
\item
  Guesses are lower than the actual crab ages.

  \begin{itemize}
  \tightlist
  \item
    Older crabs may not be harvested soon enough.
  \end{itemize}
\end{itemize}

\subparagraph{Neural Network Model
(64-32-16-8-1)}\label{neural-network-model-64-32-16-8-1}

\subparagraph{Neural Network Model
(32-16-8-1)}\label{neural-network-model-32-16-8-1}

\subparagraph{Neural Network Model
(16-8-1)}\label{neural-network-model-16-8-1}

\subparagraph{Neural Network Model
(8-1)}\label{neural-network-model-8-1}

\begin{itemize}
\tightlist
\item
  All looking good.

  \begin{itemize}
  \tightlist
  \item
    Some middle-aged crabs are guessed to be older, but this makes sense
    since crabs stop growing as much after a certain age.
  \end{itemize}
\end{itemize}

\subparagraph{Neural Network Model
(4-1)}\label{neural-network-model-4-1}

\begin{itemize}
\tightlist
\item
  Something strange going on here.

  \begin{itemize}
  \tightlist
  \item
    This model is predicting a disproportionate amount of crabs are 5
    years old.
  \end{itemize}
\end{itemize}

\subparagraph{Neural Network Model
(2-1)}\label{neural-network-model-2-1}

\begin{itemize}
\tightlist
\item
  Visually similar to the other neural network models.

  \begin{itemize}
  \tightlist
  \item
    Scores show is making predictions further from the truth.
  \end{itemize}
\end{itemize}

    \subsubsection{Training Loss Over Time
Plots}\label{training-loss-over-time-plots}

Now we'll show the training loss over time. This gives us insight into
how quickly the model is learning. It can also show us if the model is
overfitting or not.

Training loss should decrease over time, but if the validation loss
starts to increase, the model is overfitting.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}histories} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{linear\PYZus{}history}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deepest\PYZus{}history}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deep\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}history}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deep\PYZus{}16\PYZus{}8\PYZus{}history}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deep\PYZus{}8\PYZus{}history}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deep\PYZus{}4\PYZus{}history}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{deep\PYZus{}2\PYZus{}history}
\PY{p}{\PYZcb{}}

\PY{n}{plot\PYZus{}training\PYZus{}loss\PYZus{}from\PYZus{}dict}\PY{p}{(}\PY{n}{all\PYZus{}histories}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 31.2 ms
Wall time: 53.1 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_73_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Training Loss Over Time
Observations}\label{training-loss-over-time-observations}

Pretty cool, huh?

\emph{\textbf{Note}: These models have some overhead involved in
training, so it's not as simple as ``more neurons = better''. Sometimes
a simple ML algorithm can do the trick in milliseconds.}

\subparagraph{Linear Model}\label{linear-model}

\begin{itemize}
\tightlist
\item
  Never even showed up to the party.
\item
  Exceeds a Mean Squared Error of 10.
\end{itemize}

\subparagraph{Neural Network Model
(64-32-16-8-1)}\label{neural-network-model-64-32-16-8-1}

\begin{itemize}
\tightlist
\item
  Clearly overfitting already.
\item
  Gets the gist quickly.
\end{itemize}

\subparagraph{Neural Network Model
(32-16-8-1)}\label{neural-network-model-32-16-8-1}

\begin{itemize}
\tightlist
\item
  Looking good.
\item
  Also gets to the gist quickly.
\end{itemize}

\subparagraph{Neural Network Model
(16-8-1)}\label{neural-network-model-16-8-1}

\begin{itemize}
\tightlist
\item
  Similar to the (32-16-8-1) model.

  \begin{itemize}
  \tightlist
  \item
    Less variance in the training loss.
  \end{itemize}
\end{itemize}

\subparagraph{Neural Network Model
(8-1)}\label{neural-network-model-8-1}

\begin{itemize}
\tightlist
\item
  The curve is smoothing out.
\end{itemize}

\subparagraph{Neural Network Model
(4-1)}\label{neural-network-model-4-1}

\begin{itemize}
\tightlist
\item
  Not as quick to converge.
\end{itemize}

\subparagraph{Neural Network Model
(2-1)}\label{neural-network-model-2-1}

\begin{itemize}
\tightlist
\item
  Lagging behind.

  \begin{itemize}
  \tightlist
  \item
    Perhaps more epochs will give this model a chance.
  \end{itemize}
\end{itemize}

    \subsubsection{Re-Train the Models
Again}\label{re-train-the-models-again}

Let's start over, but this time for longer.

Give them 5x as many epochs this time.

    \subparagraph{Linear Model}\label{linear-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} add more epochs}
\PY{c+c1}{\PYZsh{}     common\PYZus{}fit\PYZus{}options = \PYZob{}}
\PY{c+c1}{\PYZsh{}         \PYZsq{}x\PYZsq{}: X\PYZus{}train,}
\PY{c+c1}{\PYZsh{}         \PYZsq{}y\PYZsq{}: y\PYZus{}train,}
\PY{c+c1}{\PYZsh{}         \PYZsq{}epochs\PYZsq{}: NUM\PYZus{}EPOCHS*5,}
\PY{c+c1}{\PYZsh{}         \PYZsq{}verbose\PYZsq{}: 0,}
\PY{c+c1}{\PYZsh{}         \PYZsq{}validation\PYZus{}split\PYZsq{}: VALIDATION\PYZus{}SPLIT}
\PY{c+c1}{\PYZsh{}     \PYZcb{}}
\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{NUM\PYZus{}EPOCHS}\PY{o}{*}\PY{l+m+mi}{5} \PY{c+c1}{\PYZsh{} give them 5x as many epochs}

\PY{c+c1}{\PYZsh{} reset the linear model}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}histories}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
    \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{linear\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}linear.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{all\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Linear Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 7.52 s
Wall time: 35 s
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_77_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{Neural Network Model
(64-32-16-8-1)}\label{neural-network-model-64-32-16-8-1}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} reset the (64\PYZhy{}32\PYZhy{}16\PYZhy{}8\PYZhy{}1) model}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}histories}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deepest\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{all\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZhy{}32\PYZhy{}16\PYZhy{}8\PYZhy{}1 NN Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 8.88 s
Wall time: 40.4 s
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_79_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    (64-32-16-8-1) is definitely overfitting. Let's try the next one.

    \subparagraph{Neural Network Model
(32-16-8-1)}\label{neural-network-model-32-16-8-1}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} reset the (32\PYZhy{}16\PYZhy{}8\PYZhy{}1) model}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}histories}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{all\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZhy{}16\PYZhy{}8\PYZhy{}1 NN Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 9.72 s
Wall time: 38.6 s
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_82_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    (32-16-8-1) still overfitting. Let's keep going.

    \subparagraph{Neural Network Model
(16-8-1)}\label{neural-network-model-16-8-1}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} reset the (16\PYZhy{}8\PYZhy{}1) model}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}histories}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}16\PYZus{}8\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}16\PYZus{}8.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{all\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZhy{}8\PYZhy{}1 NN Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 7.83 s
Wall time: 37.4 s
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_85_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Validation loss is remaining steady, and the training loss is decreasing
ever so slightly. It \emph{might} be overfitting, but it's hard to tell.

    \subparagraph{Neural Network Model
(8-1)}\label{neural-network-model-8-1}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} reset the (8\PYZhy{}1) model}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}histories}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{all\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 8.72 s
Wall time: 37.2 s
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_88_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    (8-1) doesn't seem to be overfitting. Let's keep it in mind.

    \subparagraph{Neural Network Model
(4-1)}\label{neural-network-model-4-1}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} reset the (4\PYZhy{}1) model}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}histories}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}4\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}4.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{all\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZhy{}1 NN Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 7.02 s
Wall time: 36.7 s
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_91_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    (4-1) Looks pretty good!

    \subparagraph{Neural Network Model
(2-1)}\label{neural-network-model-2-1}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} reset the (2\PYZhy{}1) model}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}histories}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}2\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}2.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{all\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZhy{}1 NN Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 8.72 s
Wall time: 36.7 s
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_94_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Re-Plot Training Loss Over
Time}\label{re-plot-training-loss-over-time}

Over 500 epochs, we can see how the models are learning.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss\PYZus{}from\PYZus{}dict}\PY{p}{(}\PY{n}{all\PYZus{}histories}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 31.2 ms
Wall time: 50.6 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_96_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Training Loss Over More Time
Observations}\label{training-loss-over-more-time-observations}

Cool stuff!

\subparagraph{Linear Model}\label{linear-model}

\begin{itemize}
\tightlist
\item
  Finally showed up to the party.
\item
  Shares a convergence with the (2-1) model to MSE of \textasciitilde4.
\end{itemize}

\subparagraph{Neural Network Model
(64-32-16-8-1)}\label{neural-network-model-64-32-16-8-1}

\begin{itemize}
\tightlist
\item
  Obviously overfitting.
\end{itemize}

\subparagraph{Neural Network Model
(32-16-8-1)}\label{neural-network-model-32-16-8-1}

\begin{itemize}
\tightlist
\item
  Also overfitting.
\end{itemize}

\subparagraph{Neural Network Model
(16-8-1)}\label{neural-network-model-16-8-1}

\subparagraph{Neural Network Model
(8-1)}\label{neural-network-model-8-1}

\begin{itemize}
\tightlist
\item
  Similar to the more complex neural networks.

  \begin{itemize}
  \tightlist
  \item
    Less variance as the number of neurons decreases.
  \end{itemize}
\end{itemize}

\subparagraph{Neural Network Model
(4-1)}\label{neural-network-model-4-1}

\begin{itemize}
\tightlist
\item
  After a bumpy start, it got the hang of it.

  \begin{itemize}
  \tightlist
  \item
    Less variance in the training and validation loss.
  \end{itemize}
\end{itemize}

\subparagraph{Neural Network Model
(2-1)}\label{neural-network-model-2-1}

\begin{itemize}
\tightlist
\item
  It never caught up.
\item
  But it's not overfitting so much, so that's good.
\end{itemize}

\emph{\textbf{Note}: Implementing Early Stopping on these models
resulted in early terminations in most cases.}

    \subsection{Model Leaderboard}\label{model-leaderboard}

Let's re-score the models and see how they compare.

\subsubsection{My Criteria}\label{my-criteria}

\begin{itemize}
\tightlist
\item
  Mean Absolute Error within 2 years.
\item
  Reasonable Explained Variance Score
\item
  Reasonable R2 Score
\item
  Avoid Overfitting
\item
  Reasonable Learning Rate
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} score each model}
\PY{n}{all\PYZus{}models} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{64\PYZus{}32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{32\PYZus{}16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{16\PYZus{}8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} score on the test set}
\PY{k}{for} \PY{n}{model\PYZus{}name}\PY{p}{,} \PY{n}{model} \PY{o+ow}{in} \PY{n}{all\PYZus{}models}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{preds} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
    \PY{n}{scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{model\PYZus{}name}\PY{p}{)}
    \PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{scores\PYZus{}df}\PY{p}{)}

\PY{c+c1}{\PYZsh{} copy untrained linear model scores \PYZhy{} random doesn\PYZsq{}t get another chance here for time\PYZsq{}s sake}
\PY{n}{training\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{untrained\PYZus{}linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
\PY{c+c1}{\PYZsh{} score on the training set}
\PY{k}{for} \PY{n}{model\PYZus{}name}\PY{p}{,} \PY{n}{model} \PY{o+ow}{in} \PY{n}{all\PYZus{}models}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{preds} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
    \PY{n}{scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{model\PYZus{}name}\PY{o}{+}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{training\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{training\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{scores\PYZus{}df}\PY{p}{)}

\PY{n}{leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 914us/step
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 2ms/step
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 402us/step
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 460us/step
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 484us/step
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 478us/step
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 466us/step
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 456us/step
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 463us/step
CPU times: total: 453 ms
Wall time: 1.34 s
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear          101.943787             9.748023
linear                      3.997827             1.473956
64\_32\_16\_8\_1                3.630893             1.404292
32\_16\_8\_1                   3.602257             1.385743
16\_8\_1                      3.807182             1.415440
8\_1                         3.794136             1.432214
4\_1                         3.901053             1.461622
2\_1                         3.946111             1.468480

                  explained\_variance\_score   r2\_score
untrained\_linear                  0.049686 -13.000124
linear                            0.011232   0.010781
64\_32\_16\_8\_1                      0.302562   0.302399
32\_16\_8\_1                         0.338213   0.337592
16\_8\_1                            0.280600   0.279393
8\_1                               0.228980   0.228786
4\_1                               0.178054   0.177953
2\_1                               0.044709   0.044348
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Test Set Leaderboard
Observations}\label{test-set-leaderboard-observations}

Everyone but the random model did pretty well. Let's see how they did on
the training set.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{training\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{44}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                    mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear            101.943787             9.748023
linear\_train                  3.958444             1.475843
64\_32\_16\_8\_1\_train            3.003210             1.266670
32\_16\_8\_1\_train               3.251329             1.309139
16\_8\_1\_train                  3.341644             1.325369
8\_1\_train                     3.571968             1.389613
4\_1\_train                     3.719555             1.432148
2\_1\_train                     3.923570             1.470982

                    explained\_variance\_score   r2\_score
untrained\_linear                    0.049686 -13.000124
linear\_train                        0.047926   0.047916
64\_32\_16\_8\_1\_train                  0.452759   0.452592
32\_16\_8\_1\_train                     0.419088   0.418564
16\_8\_1\_train                        0.342692   0.342082
8\_1\_train                           0.268823   0.268768
4\_1\_train                           0.233845   0.233813
2\_1\_train                           0.080701   0.080701
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Training Set Leaderboard
Observations}\label{training-set-leaderboard-observations}

Everyone did better, as expected. Hopefully, they didn't do too much
better. That would signal overfitting.

    \subsubsection{Putting it All Together}\label{putting-it-all-together}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{combined\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{training\PYZus{}leaderboard\PYZus{}df}\PY{p}{)}\PY{o}{.}\PY{n}{sort\PYZus{}index}\PY{p}{(}\PY{p}{)}
\PY{n}{combined\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 1e+03 µs
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                    mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
16\_8\_1                        3.807182             1.415440
16\_8\_1\_train                  3.341644             1.325369
2\_1                           3.946111             1.468480
2\_1\_train                     3.923570             1.470982
32\_16\_8\_1                     3.602257             1.385743
32\_16\_8\_1\_train               3.251329             1.309139
4\_1                           3.901053             1.461622
4\_1\_train                     3.719555             1.432148
64\_32\_16\_8\_1                  3.630893             1.404292
64\_32\_16\_8\_1\_train            3.003210             1.266670
8\_1                           3.794136             1.432214
8\_1\_train                     3.571968             1.389613
linear                        3.997827             1.473956
linear\_train                  3.958444             1.475843
untrained\_linear            101.943787             9.748023

                    explained\_variance\_score   r2\_score
16\_8\_1                              0.280600   0.279393
16\_8\_1\_train                        0.342692   0.342082
2\_1                                 0.044709   0.044348
2\_1\_train                           0.080701   0.080701
32\_16\_8\_1                           0.338213   0.337592
32\_16\_8\_1\_train                     0.419088   0.418564
4\_1                                 0.178054   0.177953
4\_1\_train                           0.233845   0.233813
64\_32\_16\_8\_1                        0.302562   0.302399
64\_32\_16\_8\_1\_train                  0.452759   0.452592
8\_1                                 0.228980   0.228786
8\_1\_train                           0.268823   0.268768
linear                              0.011232   0.010781
linear\_train                        0.047926   0.047916
untrained\_linear                    0.049686 -13.000124
\end{Verbatim}
\end{tcolorbox}
        
    \subsubsection{Reminder of Our Metrics}\label{reminder-of-our-metrics}

\paragraph{Mean Squared Error}\label{mean-squared-error}

\begin{itemize}
\tightlist
\item
  The best score is 0.0
\item
  Lower is better.
\end{itemize}

\paragraph{Mean Absolute Error}\label{mean-absolute-error}

\begin{itemize}
\tightlist
\item
  The best score is 0.0
\item
  Lower is better.
\item
  Less sensitive to outliers.
\end{itemize}

\paragraph{Explained Variance Score}\label{explained-variance-score}

\begin{itemize}
\tightlist
\item
  The best score is 1.0
\item
  Lower is worse.
\end{itemize}

\paragraph{R2 Score}\label{r2-score}

\begin{itemize}
\tightlist
\item
  The best score is 1.0
\item
  Lower is worse.
\end{itemize}

    \subsubsection{Model Type Comparison}\label{model-type-comparison}

\emph{\textbf{Note}: Exclude the untrained linear model from these
graphs for clarity.}

\paragraph{R2 Score}\label{r2-score}

\begin{itemize}
\tightlist
\item
  Explained Variance Score
\item
  R2 Score
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{99}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{clarified\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{untrained\PYZus{}linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r2\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{explained\PYZus{}variance\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
\PY{n}{clarified\PYZus{}leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature\PYZhy{}Rich vs Deep Learning Model R2 Scores}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 21.6 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{99}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<Axes: title=\{'center': 'Feature-Rich vs Deep Learning Model R2 Scores'\}>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_107_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Mean Squared Error}\label{mean-squared-error}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{101}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{clarified\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{untrained\PYZus{}linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}absolute\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
\PY{n}{clarified\PYZus{}leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature\PYZhy{}Rich vs Deep Learning Model MSE Scores}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 21 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{101}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<Axes: title=\{'center': 'Feature-Rich vs Deep Learning Model MSE Scores'\}>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_109_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Score Comparison
Observations}\label{score-comparison-observations}

\subparagraph{Neural Network Model
(64-32-16-8-1)}\label{neural-network-model-64-32-16-8-1}

(64-32-16-8-1) is definitely overfitting.

\subparagraph{Neural Network Model
(32-16-8-1)}\label{neural-network-model-32-16-8-1}

(32-16-8-1) is overfitting.

\subparagraph{Neural Network Model
(16-8-1)}\label{neural-network-model-16-8-1}

(16-8-1) is overfitting.

\subparagraph{Neural Network Model
(8-1)}\label{neural-network-model-8-1}

(8-1) is not overfitting too much.

\subparagraph{Neural Network Model
(4-1)}\label{neural-network-model-4-1}

(4-1) is not overfitting too much either.

\subparagraph{Neural Network Model
(2-1)}\label{neural-network-model-2-1}

(2-1) is not overfitting too much either.

    \subsubsection{Show the Leaderboard
Again}\label{show-the-leaderboard-again}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear          101.943787             9.748023
linear                      3.997827             1.473956
64\_32\_16\_8\_1                3.630893             1.404292
32\_16\_8\_1                   3.602257             1.385743
16\_8\_1                      3.807182             1.415440
8\_1                         3.794136             1.432214
4\_1                         3.901053             1.461622
2\_1                         3.946111             1.468480

                  explained\_variance\_score   r2\_score
untrained\_linear                  0.049686 -13.000124
linear                            0.011232   0.010781
64\_32\_16\_8\_1                      0.302562   0.302399
32\_16\_8\_1                         0.338213   0.337592
16\_8\_1                            0.280600   0.279393
8\_1                               0.228980   0.228786
4\_1                               0.178054   0.177953
2\_1                               0.044709   0.044348
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{On Training Data}\label{on-training-data}

Hopefully they did not do much better than their test counterparts.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{training\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                    mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear            101.943787             9.748023
linear\_train                  3.958444             1.475843
64\_32\_16\_8\_1\_train            3.003210             1.266670
32\_16\_8\_1\_train               3.251329             1.309139
16\_8\_1\_train                  3.341644             1.325369
8\_1\_train                     3.571968             1.389613
4\_1\_train                     3.719555             1.432148
2\_1\_train                     3.923570             1.470982

                    explained\_variance\_score   r2\_score
untrained\_linear                    0.049686 -13.000124
linear\_train                        0.047926   0.047916
64\_32\_16\_8\_1\_train                  0.452759   0.452592
32\_16\_8\_1\_train                     0.419088   0.418564
16\_8\_1\_train                        0.342692   0.342082
8\_1\_train                           0.268823   0.268768
4\_1\_train                           0.233845   0.233813
2\_1\_train                           0.080701   0.080701
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Score These Scores}\label{score-these-scores}

Why not?

These scores will show the level of similarity between the prediction on
the test set vs.~the training set.

This could be a good way to see if the model is overfitting or
underfitting.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{score\PYZus{}score\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}

\PY{k}{for} \PY{n}{model\PYZus{}name} \PY{o+ow}{in} \PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{index}\PY{p}{:}
    \PY{k}{if} \PY{n}{model\PYZus{}name} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{untrained\PYZus{}linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
        \PY{k}{continue}
    \PY{n}{score\PYZus{}score\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}
        \PY{n}{score\PYZus{}score\PYZus{}leaderboard\PYZus{}df}\PY{p}{,}
        \PY{n}{score\PYZus{}model}\PY{p}{(}
            \PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{[}\PY{n}{model\PYZus{}name}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{training\PYZus{}leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{model\PYZus{}name}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZus{}train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{model\PYZus{}name}
        \PY{p}{)}
    \PY{p}{)}

\PY{n}{score\PYZus{}score\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 39.1 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
              mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
linear                  0.001070             0.028775
64\_32\_16\_8\_1            0.114511             0.266424
32\_16\_8\_1               0.035529             0.147345
16\_8\_1                  0.058156             0.170098
8\_1                     0.013590             0.086148
4\_1                     0.010011             0.080656
2\_1                     0.000783             0.024347

              explained\_variance\_score  r2\_score
linear                        0.999628  0.999597
64\_32\_16\_8\_1                  0.945298  0.937982
32\_16\_8\_1                     0.982482  0.979998
16\_8\_1                        0.977551  0.971957
8\_1                           0.994594  0.993585
4\_1                           0.995934  0.995667
2\_1                           0.999759  0.999692
\end{Verbatim}
\end{tcolorbox}
        
    \subsection{Choose the Best Architecture for the
Job}\label{choose-the-best-architecture-for-the-job}

Those pesky crabs don't want us to know how old they are. We'll find out
soon enough.

First, let's choose the architecture to tune.

\subsubsection{My Criteria}\label{my-criteria}

\begin{itemize}
\tightlist
\item
  Mean Absolute Error within 2 years.
\item
  Reasonable Explained Variance Score
\item
  Reasonable R2 Score
\item
  Avoid Overfitting
\item
  Reasonable Learning Rate
\end{itemize}

Based on low MSE, high R2, and high Explained Variance, my choice is the
(8-1) neural network architecture.

\subsubsection{Pursue the (8-1) Neural Network
Architecture}\label{pursue-the-8-1-neural-network-architecture}

Let's try some hyperparameter tuning on the (8-1) neural network model.

\paragraph{Why Not the (4-1) Neural Network
Architecture?}\label{why-not-the-4-1-neural-network-architecture}

Despite the (4-1) neural network model performing better over 500
epochs, it has some strange predictions in only 100 epochs.

In the interest of time and hyperparameter tuning, we'll stick with the
(8-1) neural network model since it is good and faster to train to an
acceptable level.

    \subsection{Hyperparameter Tuning}\label{hyperparameter-tuning}

Next, we will tune the hyperparameters of the (8-1) neural network
model.

\subsubsection{Hyperparameters}\label{hyperparameters}

\begin{itemize}
\tightlist
\item
  Optimizers (adam, nadam, rmsprop, sgd, adagrad, adadelta, adamax)
\item
  Learning rates (0.1, 0.01, 0.001, 0.0001, etc.)
\item
  Loss functions (mean\_squared\_error, mean\_absolute\_error, etc.)
\end{itemize}

\subparagraph{Let's reset the number of epochs to the original
value.}\label{lets-reset-the-number-of-epochs-to-the-original-value.}

Save some time since not much progress was made with more epochs.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{100}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

    \subsubsection{Optimizer Tuning}\label{optimizer-tuning}

Next we'll try compiling the (8-1) neural network model with different
optimizers to look for any improvements.

We'll try the following optimizers:

\begin{itemize}
\tightlist
\item
  Adam
\item
  Nadam
\item
  RMSprop
\item
  Stochoastic Gradient Descent (SGD)
\item
  Adagrad
\item
  Adadelta
\item
  Adamax
\end{itemize}

\paragraph{Adam Optimizer}\label{adam-optimizer}

We have already been using the
\href{https://keras.io/api/optimizers/adam/}{Adam} optimizer, but let's
try it again to get a baseline.

Adam is a popular optimizer that combines the best of Adagrad and
RMSprop.

\begin{quote}
Adam optimization is a stochastic gradient descent method that is based
on adaptive estimation of first-order and second-order moments.
According to \href{http://arxiv.org/abs/1412.6980}{Kingma et al., 2014},
the method is ``computationally efficient, has little memory
requirement, invariant to diagonal rescaling of gradients, and is well
suited for problems that are large in terms of data/parameters''.
\end{quote}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{compile\PYZus{}options} \PY{o}{=} \PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}
\PY{n}{compile\PYZus{}options}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{compile\PYZus{}options}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}Adam\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}Adam.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}

\PY{c+c1}{\PYZsh{} initialize history dictionary}
\PY{n}{optimizer\PYZus{}histories} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PYZbs{}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}Adam\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}Adam.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{314} (1.23 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{196} (788.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 1.95 s
Wall time: 8.29 s
    \end{Verbatim}

    \subparagraph{Adam Optimizer Training Loss
Plot}\label{adam-optimizer-training-loss-plot}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (Adam)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 8.51 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_123_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{Adam Optimizer Score}\label{adam-optimizer-score}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}} \PY{c+c1}{\PYZsh{} initialize prediction dictionary}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 15.6 ms
Wall time: 84.1 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{53}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
          mean\_squared\_error  mean\_absolute\_error  explained\_variance\_score  \textbackslash{}
8\_1\_Adam            3.874307             1.433025                   0.14167

          r2\_score
8\_1\_Adam  0.139598
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Nadam Optimizer}\label{nadam-optimizer}

\href{https://keras.io/api/optimizers/Nadam/}{Nadam} is Adam with
Nesterov momentum. It should converge faster than Adam by incorporating
a look-ahead feature.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{compile\PYZus{}options} \PY{o}{=} \PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}
\PY{n}{compile\PYZus{}options}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Nadam}\PY{p}{(}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{compile\PYZus{}options}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}Nadam\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}Nadam.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}

\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PYZbs{}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}Nadam\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}Nadam.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{315} (1.24 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{197} (792.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 2.81 s
Wall time: 8.07 s
    \end{Verbatim}

    \subparagraph{Nadam Optimizer Training Loss
Plot}\label{nadam-optimizer-training-loss-plot}

It does seem to converge slightly faster than Adam.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{55}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (Nadam)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 8.01 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_129_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{Nadam Optimizer Score}\label{nadam-optimizer-score}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 15.6 ms
Wall time: 83.6 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
           mean\_squared\_error  mean\_absolute\_error  explained\_variance\_score  \textbackslash{}
8\_1\_Adam             3.874307             1.433025                  0.141670
8\_1\_Nadam            3.840628             1.450489                  0.157823

           r2\_score
8\_1\_Adam   0.139598
8\_1\_Nadam  0.156640
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{RMSprop Optimizer}\label{rmsprop-optimizer}

\href{https://keras.io/api/optimizers/rmsprop/}{RMSprop} is a good
choice for recurrent neural networks. It's similar to Adagrad, but it
uses a moving average of the squared gradient for normalization.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{57}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}RMSprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{compile\PYZus{}options} \PY{o}{=} \PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}
\PY{n}{compile\PYZus{}options}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{RMSprop}\PY{p}{(}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}RMSprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{compile\PYZus{}options}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}RMSprop\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}RMSprop.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}

\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}RMSprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PYZbs{}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}RMSprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}RMSprop\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}RMSprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}RMSprop.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}RMSprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{217} (876.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{99} (400.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 1.69 s
Wall time: 8.1 s
    \end{Verbatim}

    \subparagraph{RMSprop Optimizer Training Loss
Plot}\label{rmsprop-optimizer-training-loss-plot}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}RMSprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (RMSprop)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 15.6 ms
Wall time: 9.51 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_135_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{RMSprop Optimizer Score}\label{rmsprop-optimizer-score}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{59}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}RMSprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}RMSprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}RMSprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}RMSprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 46.9 ms
Wall time: 92.6 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{59}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
             mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
8\_1\_Adam               3.874307             1.433025
8\_1\_Nadam              3.840628             1.450489
8\_1\_RMSprop            3.843666             1.453355

             explained\_variance\_score  r2\_score
8\_1\_Adam                     0.141670  0.139598
8\_1\_Nadam                    0.157823  0.156640
8\_1\_RMSprop                  0.216134  0.215187
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Stochastic Gradient Descent (SGD)
Optimizer}\label{stochastic-gradient-descent-sgd-optimizer}

\href{https://keras.io/api/optimizers/sgd/}{SGD} is the classic
optimizer. It's a good choice for shallow networks or small datasets.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{60}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}SGD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{compile\PYZus{}options} \PY{o}{=} \PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}
\PY{n}{compile\PYZus{}options}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}SGD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{compile\PYZus{}options}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}SGD\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}SGD.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}

\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}SGD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PYZbs{}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}SGD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}SGD\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}SGD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}SGD.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}SGD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{120} (488.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{2} (12.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 1.03 s
Wall time: 7.81 s
    \end{Verbatim}

    \subparagraph{SGD Optimizer Training Loss
Plot}\label{sgd-optimizer-training-loss-plot}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{61}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}SGD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (SGD)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 7.52 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_141_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{SGD Optimizer Score}\label{sgd-optimizer-score}

That training loss is crazy. Hopefully the test scores are better.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{62}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}SGD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}SGD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}SGD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}SGD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 15.6 ms
Wall time: 88.7 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{62}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
             mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
8\_1\_Adam               3.874307             1.433025
8\_1\_Nadam              3.840628             1.450489
8\_1\_RMSprop            3.843666             1.453355
8\_1\_SGD                4.126909             1.485601

             explained\_variance\_score  r2\_score
8\_1\_Adam                     0.141670  0.139598
8\_1\_Nadam                    0.157823  0.156640
8\_1\_RMSprop                  0.216134  0.215187
8\_1\_SGD                     -0.126507 -0.131760
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Adagrad Optimizer}\label{adagrad-optimizer}

\href{https://keras.io/api/optimizers/adagrad/}{Adagrad} is a good
choice for sparse data. It adapts the learning rate based on the
frequency of features.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{63}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adagrad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{compile\PYZus{}options} \PY{o}{=} \PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}
\PY{n}{compile\PYZus{}options}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adagrad}\PY{p}{(}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adagrad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{compile\PYZus{}options}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}Adagrad\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}Adagrad.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}

\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adagrad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PYZbs{}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adagrad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}Adagrad\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adagrad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}Adagrad.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adagrad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{217} (876.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{99} (400.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 1.7 s
Wall time: 8.42 s
    \end{Verbatim}

    \subparagraph{Adagrad Optimizer Training Loss
Plot}\label{adagrad-optimizer-training-loss-plot}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{64}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adagrad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (Adagrad)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 7.51 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_147_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{Adagrad Optimizer Score}\label{adagrad-optimizer-score}

Is it just me, or did Adagrad not learn anything yet?

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{65}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adagrad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adagrad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adagrad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adagrad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 0 ns
Wall time: 88.6 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{65}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
             mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
8\_1\_Adam               3.874307             1.433025
8\_1\_Nadam              3.840628             1.450489
8\_1\_RMSprop            3.843666             1.453355
8\_1\_SGD                4.126909             1.485601
8\_1\_Adagrad           14.886531             3.162995

             explained\_variance\_score  r2\_score
8\_1\_Adam                     0.141670  0.139598
8\_1\_Nadam                    0.157823  0.156640
8\_1\_RMSprop                  0.216134  0.215187
8\_1\_SGD                     -0.126507 -0.131760
8\_1\_Adagrad                  0.271905  0.184537
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Adadelta Optimizer}\label{adadelta-optimizer}

\href{https://keras.io/api/optimizers/adadelta/}{Adadelta} is a good
choice for large datasets.

\begin{quote}
Adadelta optimization is a stochastic gradient descent method that is
based on adaptive learning rate per dimension to address two drawbacks:
- The continual decay of learning rates throughout training. - The need
for a manually selected global learning rate.
\end{quote}

If its namesake is any indication, it might not do so well here. We'll
see.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{66}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adadelta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{compile\PYZus{}options} \PY{o}{=} \PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}
\PY{n}{compile\PYZus{}options}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adadelta}\PY{p}{(}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adadelta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{compile\PYZus{}options}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}Adadelta\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}Adadelta.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}

\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adadelta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PYZbs{}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adadelta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}Adadelta\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adadelta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}Adadelta.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adadelta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{314} (1.23 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{196} (788.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 1.14 s
Wall time: 8.23 s
    \end{Verbatim}

    \subparagraph{Adadelta Optimizer Training Loss
Plot}\label{adadelta-optimizer-training-loss-plot}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{67}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adadelta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (Adadelta)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 8.51 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_153_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{Adadelta Optimizer Score}\label{adadelta-optimizer-score}

It's not looking good for Adadelta based on the training loss plot.

Maybe the scores will redeem it?

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{68}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adadelta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adadelta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adadelta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adadelta}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 31.2 ms
Wall time: 89.6 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{68}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
              mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
8\_1\_Adam                3.874307             1.433025
8\_1\_Nadam               3.840628             1.450489
8\_1\_RMSprop             3.843666             1.453355
8\_1\_SGD                 4.126909             1.485601
8\_1\_Adagrad            14.886531             3.162995
8\_1\_Adadelta           18.399206             3.530084

              explained\_variance\_score  r2\_score
8\_1\_Adam                      0.141670  0.139598
8\_1\_Nadam                     0.157823  0.156640
8\_1\_RMSprop                   0.216134  0.215187
8\_1\_SGD                      -0.126507 -0.131760
8\_1\_Adagrad                   0.271905  0.184537
8\_1\_Adadelta                  0.263624  0.094465
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Adamax Optimizer}\label{adamax-optimizer}

\href{https://keras.io/api/optimizers/adamax/}{Adamax} is a variant of
Adam based on infinity norm.

It is suited for time-variant processes, so it might not be the best
choice here. Let's try it anyway.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{69}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adamax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{compile\PYZus{}options} \PY{o}{=} \PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}
\PY{n}{compile\PYZus{}options}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Adamax}\PY{p}{(}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adamax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{compile\PYZus{}options}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}Adamax\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}Adamax.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}

\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adamax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PYZbs{}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adamax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}Adamax\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adamax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}Adamax.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adamax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{314} (1.23 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{196} (788.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 1.05 s
Wall time: 8.1 s
    \end{Verbatim}

    \subparagraph{Adamax Optimizer Training Loss
Plot}\label{adamax-optimizer-training-loss-plot}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{70}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{optimizer\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adamax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (Adamax)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 7.51 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_159_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{Adamax Optimizer Score}\label{adamax-optimizer-score}

It held up pretty well despite our initial doubts.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{71}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adamax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adamax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adamax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Adamax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 46.9 ms
Wall time: 86.1 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{71}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
              mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
8\_1\_Adam                3.874307             1.433025
8\_1\_Nadam               3.840628             1.450489
8\_1\_RMSprop             3.843666             1.453355
8\_1\_SGD                 4.126909             1.485601
8\_1\_Adagrad            14.886531             3.162995
8\_1\_Adadelta           18.399206             3.530084
8\_1\_Adamax              3.936407             1.455304

              explained\_variance\_score  r2\_score
8\_1\_Adam                      0.141670  0.139598
8\_1\_Nadam                     0.157823  0.156640
8\_1\_RMSprop                   0.216134  0.215187
8\_1\_SGD                      -0.126507 -0.131760
8\_1\_Adagrad                   0.271905  0.184537
8\_1\_Adadelta                  0.263624  0.094465
8\_1\_Adamax                    0.117124  0.117120
\end{Verbatim}
\end{tcolorbox}
        
    \subsubsection{Optimizer Decision}\label{optimizer-decision}

The results are in! Time to choose an optimizer.

We'll take the best optimizer and move on to tuning the next
hyperparameter, learning rate.

\paragraph{Optimizer Training Loss
Plots}\label{optimizer-training-loss-plots}

Let's look at the big picture by looking at all the training loss plots.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{72}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss\PYZus{}from\PYZus{}dict}\PY{p}{(}\PY{n}{optimizer\PYZus{}histories}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 15.6 ms
Wall time: 54.1 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_163_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Optimizer Leaderboard}\label{optimizer-leaderboard}

Based on these training loss plots, I'm leaning towards Adam or Nadam.

Let's compare the scores on the leaderboard again before our final
decision.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{73}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{optimizer\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{73}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
              mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
8\_1\_Adam                3.874307             1.433025
8\_1\_Nadam               3.840628             1.450489
8\_1\_RMSprop             3.843666             1.453355
8\_1\_SGD                 4.126909             1.485601
8\_1\_Adagrad            14.886531             3.162995
8\_1\_Adadelta           18.399206             3.530084
8\_1\_Adamax              3.936407             1.455304

              explained\_variance\_score  r2\_score
8\_1\_Adam                      0.141670  0.139598
8\_1\_Nadam                     0.157823  0.156640
8\_1\_RMSprop                   0.216134  0.215187
8\_1\_SGD                      -0.126507 -0.131760
8\_1\_Adagrad                   0.271905  0.184537
8\_1\_Adadelta                  0.263624  0.094465
8\_1\_Adamax                    0.117124  0.117120
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{And the Winner Is\ldots{}}\label{and-the-winner-is}

\subparagraph{Nadam!}\label{nadam}

\href{https://keras.io/api/optimizers/Nadam/}{Nadam} has the best mean
and squared errors. Its variance is not as good as Adam, but a crab's
age has some wiggle room of a year or two because of how data is
collected.

Let's tune the learning rate for Nadam next. We'll create a function
with new compile options going forward.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{74}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{k}{def} \PY{n+nf}{nadam\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{p}{:}\PY{n+nb}{float}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{loss\PYZus{}metric}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Wrapper for common\PYZus{}compile\PYZus{}options with Nadam optimizer.}

\PY{l+s+sd}{    :param learning\PYZus{}rate: learning rate for Nadam optimizer}
\PY{l+s+sd}{    :param loss\PYZus{}metric: loss metric for the model. Default is \PYZsq{}mean\PYZus{}squared\PYZus{}error\PYZsq{}.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{return} \PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}
        \PY{n}{optimizer}\PY{o}{=}\PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Nadam}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{n}{learning\PYZus{}rate}\PY{p}{)}\PY{p}{,}
        \PY{n}{loss\PYZus{}metric}\PY{o}{=}\PY{n}{loss\PYZus{}metric}
    \PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

    \subsubsection{Learning Rate Tuning}\label{learning-rate-tuning}

So far, we've been using a static learning rate of 0.001. Let's try a
few different learning rates.

\begin{itemize}
\tightlist
\item
  0.1
\item
  0.01
\item
  0.001
\item
  0.0001
\item
  Scheduled
\end{itemize}

\paragraph{Learning Rate = 0.1 (Fast
Learning)}\label{learning-rate-0.1-fast-learning}

Let's try a fast learning rate to see if it helps.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{75}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} cloning from Nadam}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{nadam\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}
\PY{p}{)}

\PY{c+c1}{\PYZsh{} initialize history dictionary}
\PY{n}{learning\PYZus{}rate\PYZus{}histories} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{315} (1.24 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{197} (792.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 1.03 s
Wall time: 8.12 s
    \end{Verbatim}

    \subparagraph{Learning Rate = 0.1 Training Loss
Plot}\label{learning-rate-0.1-training-loss-plot}

We're expecting a quick approximation and a lot of variance.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{76}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{learning\PYZus{}rate\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (LR=0.1)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 8.51 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_171_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{Learning Rate = 0.1 Score}\label{learning-rate-0.1-score}

Yikes! That's a lot of variance. Let's try a slower learning rate next.
But first, let's put it on the leaderboard.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{77}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 46.9 ms
Wall time: 98.4 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{77}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
            mean\_squared\_error  mean\_absolute\_error  explained\_variance\_score  \textbackslash{}
8\_1\_LR\_0\_1            3.761772             1.446253                  0.183375

            r2\_score
8\_1\_LR\_0\_1   0.17987
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Learning Rate = 0.01 (Less Fast
Learning)}\label{learning-rate-0.01-less-fast-learning}

Still not ``slow'' learning, but let's decelerate a bit to see if we can
address the variance.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{78}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{nadam\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}

\PY{n}{learning\PYZus{}rate\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PYZbs{}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{315} (1.24 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{197} (792.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 859 ms
Wall time: 8.03 s
    \end{Verbatim}

    \subparagraph{Learning Rate = 0.01 Training Loss
Plot}\label{learning-rate-0.01-training-loss-plot}

We're looking for a smoother curve with less variance.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{79}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{learning\PYZus{}rate\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (LR=0.01)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 9.51 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_177_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{Learning Rate = 0.01
Score}\label{learning-rate-0.01-score}

That's more like it. Let's put it on the leaderboard.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{80}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}01}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 46.9 ms
Wall time: 105 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{80}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
             mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
8\_1\_LR\_0\_1             3.761772             1.446253
8\_1\_LR\_0\_01            3.712384             1.408267

             explained\_variance\_score  r2\_score
8\_1\_LR\_0\_1                   0.183375  0.179870
8\_1\_LR\_0\_01                  0.238712  0.237596
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Learning Rate = 0.001 (Slow
Learning)}\label{learning-rate-0.001-slow-learning}

This is the learning rate we've been using, so we know what to expect.

Let's confirm our expectations and see how it compares to the others.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{81}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{nadam\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{)}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}

\PY{n}{learning\PYZus{}rate\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PYZbs{}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{315} (1.24 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{197} (792.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 1.47 s
Wall time: 8.34 s
    \end{Verbatim}

    \subparagraph{Learning Rate = 0.001 Training Loss
Plot}\label{learning-rate-0.001-training-loss-plot}

This should look familiar.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{82}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{learning\PYZus{}rate\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (LR=0.001)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 7.5 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_183_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{Learning Rate = 0.001
Score}\label{learning-rate-0.001-score}

Add it to the leaderboard.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{83}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 78.1 ms
Wall time: 93.6 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{83}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
              mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
8\_1\_LR\_0\_1              3.761772             1.446253
8\_1\_LR\_0\_01             3.712384             1.408267
8\_1\_LR\_0\_001            4.019154             1.480873

              explained\_variance\_score  r2\_score
8\_1\_LR\_0\_1                    0.183375  0.179870
8\_1\_LR\_0\_01                   0.238712  0.237596
8\_1\_LR\_0\_001                  0.136125  0.136110
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Learning Rate = 0.0001 (Slower
Learning)}\label{learning-rate-0.0001-slower-learning}

Let's slow down the learning rate even more. It might take a while to
converge, but we expect less variance.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{84}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{nadam\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.0001}\PY{p}{)}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}

\PY{n}{learning\PYZus{}rate\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PYZbs{}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{315} (1.24 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{197} (792.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 2.06 s
Wall time: 9.04 s
    \end{Verbatim}

    \subparagraph{Learning Rate = 0.0001 Training Loss
Plot}\label{learning-rate-0.0001-training-loss-plot}

This should be a slow and steady curve.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{85}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{learning\PYZus{}rate\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (LR=0.0001)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 8.51 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_189_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{Learning Rate = 0.0001
Score}\label{learning-rate-0.0001-score}

This one is acting as expected. In an ideal world, we would give every
model more epochs, but for the sake of time, we'll stick to 100 epochs
and consider this `too slow' for this project.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{86}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}0\PYZus{}0001}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{sort\PYZus{}index}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 15.6 ms
Wall time: 92.8 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{86}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
               mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
8\_1\_LR\_0\_0001            5.629026             1.729137
8\_1\_LR\_0\_001             4.019154             1.480873
8\_1\_LR\_0\_01              3.712384             1.408267
8\_1\_LR\_0\_1               3.761772             1.446253

               explained\_variance\_score  r2\_score
8\_1\_LR\_0\_0001                  0.024360  0.004204
8\_1\_LR\_0\_001                   0.136125  0.136110
8\_1\_LR\_0\_01                    0.238712  0.237596
8\_1\_LR\_0\_1                     0.183375  0.179870
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Scheduled Learning Rate}\label{scheduled-learning-rate}

Let's use what we learned from
\href{https://github.com/ahester57/ai_workshop/blob/master/docs/ANNEAL.md}{simulated
annealing} to schedule the learning rate.

Learning rate (0.01) has the best scores so far.

Our scheduled learning rate can start here and decrease by \(X\)\% every
\(Y\) epochs of no improvement.

We learned from an earlier experiment that these networks commonly
plateau but continue to learn after a while, so we want to give it a
chance to learn.

We'll use a
\href{https://keras.io/api/callbacks/reduce_lr_on_plateau/}{ReduceLROnPlateau}
callback to adjust the learning rate based on the validation loss.

\begin{itemize}
\tightlist
\item
  \emph{Factor = 0.75}: The factor by which the learning rate will be
  reduced. new\_lr = lr * factor.
\item
  \emph{Patience = 9}: Number of epochs with no improvement after which
  learning rate will be reduced.
\end{itemize}

These values were chosen based on some experimentation (not shown here
for brevity).

\emph{I wonder if we can schedule the schedule's schedule\ldots{} (We
can, but we won't here.)}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{87}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}Nadam}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{nadam\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}S\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}S.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}
\PY{p}{)}

\PY{n}{learning\PYZus{}rate\PYZus{}schedule} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ReduceLROnPlateau}\PY{p}{(}
    \PY{n}{monitor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{factor}\PY{o}{=}\PY{l+m+mf}{0.75}\PY{p}{,}
    \PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{9}\PY{p}{,}
    \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
    \PY{n}{mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min}\PY{l+s+s1}{\PYZsq{}}
\PY{p}{)}

\PY{n}{learning\PYZus{}rate\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PYZbs{}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}S\PYZus{}checkpoint}\PY{p}{,} \PY{n}{learning\PYZus{}rate\PYZus{}schedule}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}LR\PYZus{}S.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

Epoch 75: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 89: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{315} (1.24 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{197} (792.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 2.08 s
Wall time: 7.96 s
    \end{Verbatim}

    \subparagraph{Learning Rate Schedule Training Loss
Plot}\label{learning-rate-schedule-training-loss-plot}

Let's look for an improvement in the training loss rate over epochs.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{88}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{learning\PYZus{}rate\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (LR=Scheduled)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 7.51 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_195_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subparagraph{Scheduled Learning Rate
Score}\label{scheduled-learning-rate-score}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{89}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 15.6 ms
Wall time: 85.1 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{89}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
               mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
8\_1\_LR\_0\_1               3.761772             1.446253
8\_1\_LR\_0\_01              3.712384             1.408267
8\_1\_LR\_0\_001             4.019154             1.480873
8\_1\_LR\_0\_0001            5.629026             1.729137
8\_1\_LR\_S                 3.750417             1.413749

               explained\_variance\_score  r2\_score
8\_1\_LR\_0\_1                     0.183375  0.179870
8\_1\_LR\_0\_01                    0.238712  0.237596
8\_1\_LR\_0\_001                   0.136125  0.136110
8\_1\_LR\_0\_0001                  0.024360  0.004204
8\_1\_LR\_S                       0.159442  0.158437
\end{Verbatim}
\end{tcolorbox}
        
    The scheduled learning rate has the best error stats so far. But not so
fast, let's take a look at the big picture.

\subsubsection{Learning Rate Decision}\label{learning-rate-decision}

Let's compare the training loss plots and the leaderboard scores for all
the learning rates.

Reminder of our criteria:

\begin{itemize}
\tightlist
\item
  Mean Absolute Error within 2 years.
\item
  Reasonable Explained Variance Score
\item
  Reasonable R2 Score
\item
  Avoid Overfitting
\item
  Reasonable Learning Rate
\end{itemize}

\paragraph{Learning Rate Training Loss
Plots}\label{learning-rate-training-loss-plots}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{90}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss\PYZus{}from\PYZus{}dict}\PY{p}{(}\PY{n}{learning\PYZus{}rate\PYZus{}histories}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 15.6 ms
Wall time: 37.5 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_199_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Learning Rate Leaderboard}\label{learning-rate-leaderboard}

Check the leaderboard again before the big decision.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{91}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{91}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
               mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
8\_1\_LR\_0\_1               3.761772             1.446253
8\_1\_LR\_0\_01              3.712384             1.408267
8\_1\_LR\_0\_001             4.019154             1.480873
8\_1\_LR\_0\_0001            5.629026             1.729137
8\_1\_LR\_S                 3.750417             1.413749

               explained\_variance\_score  r2\_score
8\_1\_LR\_0\_1                     0.183375  0.179870
8\_1\_LR\_0\_01                    0.238712  0.237596
8\_1\_LR\_0\_001                   0.136125  0.136110
8\_1\_LR\_0\_0001                  0.024360  0.004204
8\_1\_LR\_S                       0.159442  0.158437
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{And the Winner Is\ldots{}}\label{and-the-winner-is}

\subparagraph{Scheduled Learning Rate!}\label{scheduled-learning-rate}

Spending a little extra time on the learning rate paid off. It has the
best error scores and an acceptable variance.

Specifically, we are using
\href{https://keras.io/api/callbacks/reduce_lr_on_plateau/}{ReduceLROnPlateau}
for our schedule.

\begin{itemize}
\tightlist
\item
  \emph{Factor = 0.75}: The factor by which the learning rate will be
  reduced. new\_lr = lr * factor.
\item
  \emph{Patience = 9}: Number of epochs with no improvement after which
  learning rate will be reduced.
\end{itemize}

Others exist (like
\href{https://keras.io/api/optimizers/schedules/exponential_decay/}{ExponentialDecay}),
but this one worked for us this time.

So far we have chosen the (8-1) neural network architecture with the
Nadam optimizer and a scheduled learning rate.

    \subsubsection{Loss Function to Mean Absolute
Error}\label{loss-function-to-mean-absolute-error}

Let's try a different loss function to see if it improves the model.

\begin{itemize}
\tightlist
\item
  \textbf{Loss Function}

  \begin{itemize}
  \tightlist
  \item
    Mean Absolute Error (MAE)

    \begin{itemize}
    \tightlist
    \item
      Less sensitive to outliers.
    \item
      Penalizes all errors equally.
    \end{itemize}
  \end{itemize}
\end{itemize}

This could be good for our model, as we removed outliers in the data
cleaning step. Let's find out.

We'll keep the best architecture so far and change the loss function to
MAE.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{92}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{models}\PY{o}{.}\PY{n}{clone\PYZus{}model}\PY{p}{(}\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{nadam\PYZus{}compile\PYZus{}options}\PY{p}{(}
    \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,}
    \PY{n}{loss\PYZus{}metric}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}absolute\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}MAE\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}MAE.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}
\PY{p}{)}

\PY{n}{loss\PYZus{}histories} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} 
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{deep\PYZus{}8\PYZus{}1\PYZus{}MAE\PYZus{}checkpoint}\PY{p}{,} \PY{n}{learning\PYZus{}rate\PYZus{}schedule}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}8\PYZus{}1\PYZus{}MAE.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

Epoch 57: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 69: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 86: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 95: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_4"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})   │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_13 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_14 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{315} (1.24 KB)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{197} (792.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 1.72 s
Wall time: 8.06 s
    \end{Verbatim}

    \paragraph{Lost Function Mean Absolute Error Training Loss
Plot}\label{lost-function-mean-absolute-error-training-loss-plot}

\emph{\textbf{Note}: The loss function is different, so the scale will
be different.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{93}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{loss\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZhy{}1 NN Model (MAE)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}lim}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 8.52 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{models_files/models_206_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Loss Function = Mean Absolute Error
Score}\label{loss-function-mean-absolute-error-score}

We can't tell anything yet since it's a new scale. Let's check out the
leaderboard with all the metrics.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{94}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{chosen\PYZus{}arch\PYZus{}preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{loss\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{learning\PYZus{}rate\PYZus{}leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{8\PYZus{}1\PYZus{}LR\PYZus{}S}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{deep\PYZus{}model\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{loss\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 31.2 ms
Wall time: 91.2 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{94}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
          mean\_squared\_error  mean\_absolute\_error  explained\_variance\_score  \textbackslash{}
8\_1\_LR\_S            3.750417             1.413749                  0.159442
8\_1\_MAE             3.897357             1.405759                  0.208332

          r2\_score
8\_1\_LR\_S  0.158437
8\_1\_MAE   0.201616
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Mean Absolute Error Loss Function
Observations}\label{mean-absolute-error-loss-function-observations}

Not the improvement we were looking for. Consistently lagging behind the
model trained with Mean Squared Error (MSE) loss.

    \subsubsection{Perhaps an Ensemble Will
Help}\label{perhaps-an-ensemble-will-help}

But I'm running out of time. Let's move on to feature engineering with
our best model so far.

\subsection{Winner, Winner, Crab's for
Dinner!}\label{winner-winner-crabs-for-dinner}

Reminder of our criteria:

\begin{itemize}
\tightlist
\item
  Mean Absolute Error within 2 years.
\item
  Reasonable Explained Variance Score
\item
  Reasonable R2 Score
\item
  Avoid Overfitting
\item
  Reasonable Learning Rate
\end{itemize}

\subsubsection{Our Best Model So Far}\label{our-best-model-so-far}

\begin{itemize}
\tightlist
\item
  Architecture: (8-1) Neural Network
\item
  Optimizer: Nadam
\item
  Learning Rate: Scheduled

  \begin{itemize}
  \tightlist
  \item
    Start = 0.01
  \item
    Factor = 0.75
  \item
    Patience = 9 epochs
  \end{itemize}
\item
  Loss Function: Mean Squared Error
\end{itemize}

This model should be quick to train to an acceptable level.

    \subsection{Graduate Student Section}\label{graduate-student-section}

\subsubsection{Add Output as Additional Input
Feature}\label{add-output-as-additional-input-feature}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  discuss what architecture (how big) you do need to overfit when you
  have output as additional input feature.
\end{enumerate}
\end{quote}

Let's start with linear model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{107}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} layer: input}
\PY{n}{output\PYZus{}as\PYZus{}input\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{layer\PYZus{}output\PYZus{}as\PYZus{}input\PYZus{}input} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{output\PYZus{}as\PYZus{}input\PYZus{}df}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{,}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} layer: normalizer}
\PY{n}{layer\PYZus{}output\PYZus{}as\PYZus{}input\PYZus{}normalizer} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Normalization}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{layer\PYZus{}output\PYZus{}as\PYZus{}input\PYZus{}normalizer}\PY{o}{.}\PY{n}{adapt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{output\PYZus{}as\PYZus{}input\PYZus{}df}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} layer: output (linear regression)}
\PY{n}{layer\PYZus{}output\PYZus{}as\PYZus{}input\PYZus{}output} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} architecture:}
\PY{c+c1}{\PYZsh{}   input \PYZhy{}\PYZgt{} normalizer \PYZhy{}\PYZgt{} linear}
\PY{c+c1}{\PYZsh{} initialize the all\PYZus{}models dictionary}
\PY{n}{all\PYZus{}models} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear\PYZus{}add}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
    \PY{n}{layer\PYZus{}output\PYZus{}as\PYZus{}input\PYZus{}input}\PY{p}{,}
    \PY{n}{layer\PYZus{}output\PYZus{}as\PYZus{}input\PYZus{}normalizer}\PY{p}{,}
    \PY{n}{layer\PYZus{}output\PYZus{}as\PYZus{}input\PYZus{}output}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} compile options}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear\PYZus{}add}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} checkpoint options}
\PY{n}{linear\PYZus{}add\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}linear\PYZus{}add.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}

\PY{c+c1}{\PYZsh{} fit options}
\PY{n}{all\PYZus{}histories} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear\PYZus{}add}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
        \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear\PYZus{}add}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
            \PY{n}{x}\PY{o}{=}\PY{n}{output\PYZus{}as\PYZus{}input\PYZus{}df}\PY{p}{,}
            \PY{n}{y}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{,}
            \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,}
            \PY{n}{epochs}\PY{o}{=}\PY{n}{NUM\PYZus{}EPOCHS}\PY{p}{,}
            \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{linear\PYZus{}add\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear\PYZus{}add}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}linear\PYZus{}add.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} score the model}
\PY{n}{preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear\PYZus{}add}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{output\PYZus{}as\PYZus{}input\PYZus{}df}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear\PYZus{}add}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{scores\PYZus{}df}\PY{p}{)}

\PY{c+c1}{\PYZsh{} summary}
\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear\PYZus{}add}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step - loss:
105.6924 - val\_loss: 103.0483
Epoch 2/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 820us/step -
loss: 101.6268 - val\_loss: 99.4652
Epoch 3/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 822us/step -
loss: 98.9061 - val\_loss: 96.5345
Epoch 4/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 807us/step -
loss: 92.7014 - val\_loss: 94.0602
Epoch 5/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 879us/step -
loss: 93.7745 - val\_loss: 91.8188
Epoch 6/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 827us/step -
loss: 90.0358 - val\_loss: 89.7515
Epoch 7/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 869us/step -
loss: 88.9988 - val\_loss: 87.7492
Epoch 8/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 896us/step -
loss: 86.2676 - val\_loss: 85.8454
Epoch 9/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 797us/step -
loss: 84.5390 - val\_loss: 83.9554
Epoch 10/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 745us/step -
loss: 82.5307 - val\_loss: 82.1252
Epoch 11/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 733us/step -
loss: 80.4340 - val\_loss: 80.3183
Epoch 12/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 721us/step -
loss: 79.4054 - val\_loss: 78.5483
Epoch 13/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 727us/step -
loss: 77.1459 - val\_loss: 76.8206
Epoch 14/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 751us/step -
loss: 76.3178 - val\_loss: 75.1000
Epoch 15/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 725us/step -
loss: 74.9121 - val\_loss: 73.4146
Epoch 16/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 727us/step -
loss: 72.1253 - val\_loss: 71.7520
Epoch 17/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 730us/step -
loss: 70.0902 - val\_loss: 70.1322
Epoch 18/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 746us/step -
loss: 69.9480 - val\_loss: 68.5168
Epoch 19/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 724us/step -
loss: 67.6865 - val\_loss: 66.9430
Epoch 20/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 721us/step -
loss: 65.6243 - val\_loss: 65.4002
Epoch 21/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 747us/step -
loss: 64.5850 - val\_loss: 63.8779
Epoch 22/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 719us/step -
loss: 63.7362 - val\_loss: 62.3700
Epoch 23/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 716us/step -
loss: 62.4219 - val\_loss: 60.8996
Epoch 24/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 724us/step -
loss: 60.0935 - val\_loss: 59.4564
Epoch 25/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 735us/step -
loss: 58.9865 - val\_loss: 58.0260
Epoch 26/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 765us/step -
loss: 57.8530 - val\_loss: 56.6237
Epoch 27/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 762us/step -
loss: 56.0950 - val\_loss: 55.2473
Epoch 28/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 772us/step -
loss: 54.2189 - val\_loss: 53.8912
Epoch 29/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 764us/step -
loss: 53.5180 - val\_loss: 52.5589
Epoch 30/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 761us/step -
loss: 52.7011 - val\_loss: 51.2514
Epoch 31/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 762us/step -
loss: 50.6838 - val\_loss: 49.9611
Epoch 32/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 772us/step -
loss: 49.8138 - val\_loss: 48.6989
Epoch 33/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 760us/step -
loss: 48.5863 - val\_loss: 47.4573
Epoch 34/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 753us/step -
loss: 47.1432 - val\_loss: 46.2341
Epoch 35/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 779us/step -
loss: 46.1972 - val\_loss: 45.0253
Epoch 36/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 786us/step -
loss: 44.6612 - val\_loss: 43.8500
Epoch 37/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 774us/step -
loss: 44.1122 - val\_loss: 42.6881
Epoch 38/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 754us/step -
loss: 42.0755 - val\_loss: 41.5520
Epoch 39/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 772us/step -
loss: 41.0877 - val\_loss: 40.4250
Epoch 40/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 770us/step -
loss: 40.0457 - val\_loss: 39.3242
Epoch 41/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 730us/step -
loss: 39.3890 - val\_loss: 38.2409
Epoch 42/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 731us/step -
loss: 37.9221 - val\_loss: 37.1801
Epoch 43/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 758us/step -
loss: 36.6264 - val\_loss: 36.1374
Epoch 44/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 736us/step -
loss: 35.8344 - val\_loss: 35.1099
Epoch 45/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 746us/step -
loss: 35.1291 - val\_loss: 34.0990
Epoch 46/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 723us/step -
loss: 34.0102 - val\_loss: 33.1134
Epoch 47/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 753us/step -
loss: 32.8082 - val\_loss: 32.1422
Epoch 48/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 742us/step -
loss: 31.7240 - val\_loss: 31.1906
Epoch 49/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 730us/step -
loss: 31.0992 - val\_loss: 30.2542
Epoch 50/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 743us/step -
loss: 30.1142 - val\_loss: 29.3367
Epoch 51/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 741us/step -
loss: 28.9096 - val\_loss: 28.4392
Epoch 52/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 726us/step -
loss: 28.1678 - val\_loss: 27.5567
Epoch 53/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 726us/step -
loss: 27.2593 - val\_loss: 26.6891
Epoch 54/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 732us/step -
loss: 26.4221 - val\_loss: 25.8409
Epoch 55/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 740us/step -
loss: 25.5427 - val\_loss: 25.0096
Epoch 56/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 746us/step -
loss: 24.8449 - val\_loss: 24.1942
Epoch 57/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 774us/step -
loss: 23.9737 - val\_loss: 23.3993
Epoch 58/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 759us/step -
loss: 23.0761 - val\_loss: 22.6121
Epoch 59/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 762us/step -
loss: 22.2593 - val\_loss: 21.8457
Epoch 60/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 763us/step -
loss: 21.5584 - val\_loss: 21.0942
Epoch 61/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 761us/step -
loss: 20.8564 - val\_loss: 20.3633
Epoch 62/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 765us/step -
loss: 20.2320 - val\_loss: 19.6400
Epoch 63/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 776us/step -
loss: 19.3908 - val\_loss: 18.9421
Epoch 64/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 782us/step -
loss: 18.7075 - val\_loss: 18.2533
Epoch 65/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 761us/step -
loss: 18.0204 - val\_loss: 17.5802
Epoch 66/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 830us/step -
loss: 17.4017 - val\_loss: 16.9253
Epoch 67/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 787us/step -
loss: 16.7196 - val\_loss: 16.2821
Epoch 68/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 750us/step -
loss: 16.1802 - val\_loss: 15.6558
Epoch 69/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 790us/step -
loss: 15.5185 - val\_loss: 15.0433
Epoch 70/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 767us/step -
loss: 14.8515 - val\_loss: 14.4478
Epoch 71/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 721us/step -
loss: 14.3270 - val\_loss: 13.8643
Epoch 72/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 718us/step -
loss: 13.7105 - val\_loss: 13.2988
Epoch 73/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 713us/step -
loss: 13.2021 - val\_loss: 12.7420
Epoch 74/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 726us/step -
loss: 12.6565 - val\_loss: 12.2037
Epoch 75/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 750us/step -
loss: 12.0777 - val\_loss: 11.6787
Epoch 76/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 727us/step -
loss: 11.6135 - val\_loss: 11.1660
Epoch 77/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 736us/step -
loss: 11.0596 - val\_loss: 10.6707
Epoch 78/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 729us/step -
loss: 10.5589 - val\_loss: 10.1859
Epoch 79/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 729us/step -
loss: 10.0412 - val\_loss: 9.7176
Epoch 80/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 726us/step -
loss: 9.6421 - val\_loss: 9.2591
Epoch 81/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 735us/step -
loss: 9.1733 - val\_loss: 8.8159
Epoch 82/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 719us/step -
loss: 8.7153 - val\_loss: 8.3875
Epoch 83/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 722us/step -
loss: 8.3182 - val\_loss: 7.9693
Epoch 84/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 714us/step -
loss: 7.8751 - val\_loss: 7.5651
Epoch 85/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 725us/step -
loss: 7.4688 - val\_loss: 7.1756
Epoch 86/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 728us/step -
loss: 7.0620 - val\_loss: 6.7965
Epoch 87/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 730us/step -
loss: 6.7240 - val\_loss: 6.4305
Epoch 88/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 777us/step -
loss: 6.3739 - val\_loss: 6.0780
Epoch 89/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 779us/step -
loss: 6.0006 - val\_loss: 5.7369
Epoch 90/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 771us/step -
loss: 5.6543 - val\_loss: 5.4080
Epoch 91/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 774us/step -
loss: 5.3181 - val\_loss: 5.0927
Epoch 92/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 770us/step -
loss: 5.0447 - val\_loss: 4.7870
Epoch 93/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 767us/step -
loss: 4.7329 - val\_loss: 4.4951
Epoch 94/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 751us/step -
loss: 4.4401 - val\_loss: 4.2132
Epoch 95/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 778us/step -
loss: 4.1466 - val\_loss: 3.9434
Epoch 96/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 750us/step -
loss: 3.8473 - val\_loss: 3.6851
Epoch 97/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 773us/step -
loss: 3.6259 - val\_loss: 3.4371
Epoch 98/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 760us/step -
loss: 3.3756 - val\_loss: 3.2025
Epoch 99/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 794us/step -
loss: 3.1554 - val\_loss: 2.9755
Epoch 100/100
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 780us/step -
loss: 2.9205 - val\_loss: 2.7600
\textbf{95/95} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 544us/step
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_10"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization\_4 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization}) │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{11})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{23} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_22 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})                │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{12} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{61} (252.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{12} (48.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{23} (96.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Optimizer params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{26} (108.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 891 ms
Wall time: 9.35 s
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{112}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} show the scores}
\PY{n}{leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{112}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                  mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
untrained\_linear          101.943787             9.748023
linear                      3.997827             1.473956
64\_32\_16\_8\_1                3.630893             1.404292
32\_16\_8\_1                   3.602257             1.385743
16\_8\_1                      3.807182             1.415440
8\_1                         3.794136             1.432214
4\_1                         3.901053             1.461622
2\_1                         3.946111             1.468480
linear\_add                  2.769430             1.654084

                  explained\_variance\_score   r2\_score
untrained\_linear                  0.049686 -13.000124
linear                            0.011232   0.010781
64\_32\_16\_8\_1                      0.302562   0.302399
32\_16\_8\_1                         0.338213   0.337592
16\_8\_1                            0.280600   0.279393
8\_1                               0.228980   0.228786
4\_1                               0.178054   0.177953
2\_1                               0.044709   0.044348
linear\_add                        0.996007   0.669256
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Linear Model with Output as Additional Input
Observations}\label{linear-model-with-output-as-additional-input-observations}

Look at that variance! It's overfitting like a mad crab.

It took only a single linear layer to overfit the model. This is a good
example of why we need to be careful about data leakage.

    \subsubsection{Model as Code}\label{model-as-code}

\ldots and it's not working. Worth a shot.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{128}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}

\PY{k}{def} \PY{n+nf}{relu}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{x}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{n}{layers}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Initialize input}
    \PY{n}{layer\PYZus{}input} \PY{o}{=} \PY{n}{X}

    \PY{c+c1}{\PYZsh{} Iterate over layers}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{layers}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Apply weights to input and add bias}
        \PY{n}{layer\PYZus{}output} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{layer\PYZus{}input}\PY{p}{,} \PY{n}{weights}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{n}{weights}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}

        \PY{c+c1}{\PYZsh{} Apply activation function}
        \PY{n}{layer\PYZus{}output} \PY{o}{=} \PY{n}{layer\PYZus{}output}

        \PY{c+c1}{\PYZsh{} Update input for next layer}
        \PY{n}{layer\PYZus{}input} \PY{o}{=} \PY{n}{layer\PYZus{}output}

    \PY{c+c1}{\PYZsh{} Return output of final layer as predictions}
    \PY{k}{return} \PY{n}{layer\PYZus{}output}


\PY{c+c1}{\PYZsh{} get the weights}
\PY{n}{weights} \PY{o}{=} \PY{p}{[}\PY{n}{layer}\PY{o}{.}\PY{n}{get\PYZus{}weights}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{layer} \PY{o+ow}{in} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear\PYZus{}add}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{layers}\PY{p}{]}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{weights}\PY{p}{)}

\PY{c+c1}{\PYZsh{} predict}
\PY{n}{preds} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{weights}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{output\PYZus{}as\PYZus{}input\PYZus{}df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{preds}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[[array([ 1.3032532 ,  1.0129274 ,  0.34452584, 23.003716  , 10.014996  ,
        5.0239677 ,  6.6078315 ,  0.3064995 ,  0.3253052 ,  0.36819533,
        9.732101  ], dtype=float32), array([8.8651031e-02, 6.0300808e-02,
8.6254207e-03, 1.8041740e+02,
       3.7331261e+01, 9.0801477e+00, 1.4141748e+01, 2.1255755e-01,
       2.1948172e-01, 2.3262753e-01, 8.4085999e+00], dtype=float32), 0],
[array([[-0.20978996],
       [ 0.351122  ],
       [-0.02942573],
       [-0.5934581 ],
       [-0.2053751 ],
       [ 0.28914857],
       [ 0.5067023 ],
       [ 0.04240793],
       [ 0.04285356],
       [ 0.06395935],
       [ 2.7610233 ]], dtype=float32), array([8.078017], dtype=float32)]]
[4990.3853]
CPU times: total: 0 ns
Wall time: 1.01 ms
    \end{Verbatim}

    \paragraph{That Doesn't Seem Right}\label{that-doesnt-seem-right}

Oh well, we tried. Let's move on to the next step.

    \subsection{No Need to Save the Data}\label{no-need-to-save-the-data}

We didn't make any changes to the data, so we can pick this back up on
the \href{../2-features/features.ipynb}{next step}.

    \subsection{Onwards to Feature
Engineering}\label{onwards-to-feature-engineering}

See the \href{../2-features/features.ipynb}{next section} for feature
engineering.

\href{https://nbviewer.org/github/ahester57/ai_workshop/blob/master/notebooks/time_for_crab/2-features/features.ipynb}{\texttt{\textless{}html\ link\textgreater{}}}
for feature reduction.
\href{../2-features/features.html}{\texttt{\textless{}localhost\ html\ link\textgreater{}}}
for feature reduction.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}

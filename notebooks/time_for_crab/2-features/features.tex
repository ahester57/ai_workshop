\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{features}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \section{Feature Selection and
Engineering}\label{feature-selection-and-engineering}

\subparagraph{\texorpdfstring{\emph{In which we boost, combine, split,
or otherwise manipulate the features of
crabs.}}{In which we boost, combine, split, or otherwise manipulate the features of crabs.}}\label{in-which-we-boost-combine-split-or-otherwise-manipulate-the-features-of-crabs.}

\href{https://github.com/ahester57/ai_workshop/tree/master/notebooks/time_for_crab/2-features}{GitHub
Repository}

\href{https://nbviewer.jupyter.org/github/ahester57/ai_workshop/blob/master/notebooks/time_for_crab/2-features/features.ipynb}{Notebook
Viewer}

\href{https://www.kaggle.com/sidhus/crab-age-prediction}{Kaggle Dataset}

    \subsection{Table of Contents}\label{table-of-contents}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \hyperref[introduction]{Introduction}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \hyperref[define-constants]{Define Constants}
  \item
    \hyperref[import-libraries]{Import Libraries}
  \item
    \hyperref[load-data-from-cache]{Load Data from Cache}
  \end{enumerate}
\item
  \hyperref[rebuild-our-model]{Rebuild our Model}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \hyperref[neural-network-architecture]{Neural Network Architecture}
  \item
    \hyperref[configure-the-neural-network-model]{Configure the Neural Network Model}
  \item
    \hyperref[train-the-model-with-full-features]{Train the Model with Full Features}
  \item
    \hyperref[score-the-model-with-full-features]{Score the Model with Full Features}
  \end{enumerate}
\item
  \hyperref[feature-analysis]{Feature Analysis}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \hyperref[distributions-of-features]{Distributions of Features}
  \item
    \hyperref[correlation-matrix]{Correlation Matrix}
  \end{enumerate}
\item
  \hyperref[feature-selection]{Feature Selection}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \hyperref[recursive-feature-elimination-rfe]{Recursive Feature Elimination (RFE)}
  \item
    \hyperref[eliminate-the-worst-features]{Eliminate the Worst Features}
  \item
    \hyperref[feature-selection-observations]{Feature Selection Observations}
  \end{enumerate}
\item
  \hyperref[feature-engineering]{Feature Engineering}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \hyperref[feature-reduction]{Feature Reduction}
  \end{enumerate}
\item
  \hyperref[save-the-data]{Save the Data}
\item
  \hyperref[onwards-to-final-evaluation]{Onwards to Final Evaluation}
\end{enumerate}

    \subsection{Introduction}\label{introduction}

Crabs are complex creatures. Let's engineer some features to help our
model find the best crabs for harvest.

We'll need to use domain knowledge to extract more features from our
dataset's column.

\begin{figure}
\centering
\includegraphics{https://i.kym-cdn.com/photos/images/newsfeed/000/112/843/killcrab.jpg}
\caption{This kills the crab.}
\end{figure}

For example, we can find the edible weight of the crab by subtracting
the viscera weight from the shucked weight.

However, we need to be careful not to overfit the model by adding
collinear features.

    \subsubsection{Define Constants}\label{define-constants}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{105}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{CACHE\PYZus{}FILE} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../cache/splitcrabs.feather}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{NEXT\PYZus{}CACHE\PYZus{}FILE} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../cache/designrcrabs.feather}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{NEXT\PYZus{}NOTEBOOK} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../3\PYZhy{}evaluation/evaluation.ipynb}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../cache/designr\PYZus{}model.weights.h5}\PY{l+s+s1}{\PYZsq{}}

\PY{n}{PREDICTION\PYZus{}TARGET} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}    \PY{c+c1}{\PYZsh{} \PYZsq{}Age\PYZsq{} is predicted}
\PY{n}{DATASET\PYZus{}COLUMNS} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sex\PYZus{}F}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sex\PYZus{}M}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sex\PYZus{}I}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Diameter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Height}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shucked Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Viscera Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shell Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}
\PY{n}{REQUIRED\PYZus{}COLUMNS} \PY{o}{=} \PY{p}{[}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}

\PY{n}{NUM\PYZus{}EPOCHS} \PY{o}{=} \PY{l+m+mi}{100}
\PY{n}{VALIDATION\PYZus{}SPLIT} \PY{o}{=} \PY{l+m+mf}{0.2}
\PY{n}{NUM\PYZus{}HIDDEN\PYZus{}LAYERS} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{NUM\PYZus{}UNITS} \PY{o}{=} \PY{l+m+mi}{8}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

    \subsubsection{Import Libraries}\label{import-libraries}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{106}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{k+kn}{from} \PY{n+nn}{notebooks}\PY{n+nn}{.}\PY{n+nn}{time\PYZus{}for\PYZus{}crab}\PY{n+nn}{.}\PY{n+nn}{mlutils} \PY{k+kn}{import} \PY{n}{display\PYZus{}df}\PY{p}{,} \PY{n}{generate\PYZus{}neural\PYZus{}network}\PY{p}{,} \PY{n}{generate\PYZus{}neural\PYZus{}pyramid}
\PY{k+kn}{from} \PY{n+nn}{notebooks}\PY{n+nn}{.}\PY{n+nn}{time\PYZus{}for\PYZus{}crab}\PY{n+nn}{.}\PY{n+nn}{mlutils} \PY{k+kn}{import} \PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{,} \PY{n}{plot\PYZus{}training\PYZus{}loss\PYZus{}from\PYZus{}dict}\PY{p}{,} \PY{n}{plot\PYZus{}true\PYZus{}vs\PYZus{}pred\PYZus{}from\PYZus{}dict}
\PY{k+kn}{from} \PY{n+nn}{notebooks}\PY{n+nn}{.}\PY{n+nn}{time\PYZus{}for\PYZus{}crab}\PY{n+nn}{.}\PY{n+nn}{mlutils} \PY{k+kn}{import} \PY{n}{score\PYZus{}combine}\PY{p}{,} \PY{n}{score\PYZus{}comparator}\PY{p}{,} \PY{n}{score\PYZus{}model}

\PY{k+kn}{import} \PY{n+nn}{keras}

\PY{n}{keras\PYZus{}backend} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{backend}\PY{o}{.}\PY{n}{backend}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Keras version: }\PY{l+s+si}{\PYZob{}}\PY{n}{keras}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Keras backend: }\PY{l+s+si}{\PYZob{}}\PY{n}{keras\PYZus{}backend}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{if} \PY{n}{keras\PYZus{}backend} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tensorflow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TensorFlow version: }\PY{l+s+si}{\PYZob{}}\PY{n}{tf}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TensorFlow devices: }\PY{l+s+si}{\PYZob{}}\PY{n}{tf}\PY{o}{.}\PY{n}{config}\PY{o}{.}\PY{n}{list\PYZus{}physical\PYZus{}devices}\PY{p}{(}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{elif} \PY{n}{keras\PYZus{}backend} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{torch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{k+kn}{import} \PY{n+nn}{torch}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Torch version: }\PY{l+s+si}{\PYZob{}}\PY{n}{torch}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Torch devices: }\PY{l+s+si}{\PYZob{}}\PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{get\PYZus{}device\PYZus{}name}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{current\PYZus{}device}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} torch supports windows\PYZhy{}native cuda, but CPU was faster for this task}
\PY{k}{elif} \PY{n}{keras\PYZus{}backend} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{jax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{k+kn}{import} \PY{n+nn}{jax}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{JAX version: }\PY{l+s+si}{\PYZob{}}\PY{n}{jax}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{JAX devices: }\PY{l+s+si}{\PYZob{}}\PY{n}{jax}\PY{o}{.}\PY{n}{devices}\PY{p}{(}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unknown backend; Proceed with caution.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}

\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{o}{\PYZpc{}}\PY{n}{matplotlib} \PY{n}{inline}
\PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}

\PY{n}{pd}\PY{o}{.}\PY{n}{set\PYZus{}option}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mode.copy\PYZus{}on\PYZus{}write}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Keras version: 3.3.3
Keras backend: tensorflow
TensorFlow version: 2.16.1
TensorFlow devices: [PhysicalDevice(name='/physical\_device:CPU:0',
device\_type='CPU')]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

    \subsubsection{Load Data from Cache}\label{load-data-from-cache}

In the \href{../1-models/models.ipynb}{model selection section}, we
grabbed the data from the cache. Let's load it back up.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{107}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{crabs} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}feather}\PY{p}{(}\PY{n}{CACHE\PYZus{}FILE}\PY{p}{)}
\PY{n}{crabs\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}feather}\PY{p}{(}\PY{n}{CACHE\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.feather}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}test.feather}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{n}{display\PYZus{}df}\PY{p}{(}\PY{n}{crabs}\PY{p}{,} \PY{n}{show\PYZus{}distinct}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} split features from target}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{crabs}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{crabs}\PY{p}{[}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}

\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{crabs\PYZus{}test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{crabs\PYZus{}test}\PY{p}{[}\PY{n}{PREDICTION\PYZus{}TARGET}\PY{p}{]}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}train shape: }\PY{l+s+si}{\PYZob{}}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}test shape: }\PY{l+s+si}{\PYZob{}}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
DataFrame shape: (3031, 11)
First 5 rows:
        Length  Diameter    Height    Weight  Shucked Weight  Viscera Weight  \textbackslash{}
3483  1.724609  1.312500  0.500000  50.53125       25.984375        9.429688
993   1.612305  1.312500  0.500000  41.09375       17.031250        7.273438
1427  1.650391  1.262695  0.475098  40.78125       19.203125        8.078125
3829  1.362305  1.150391  0.399902  25.43750        9.664062        4.691406
1468  1.250000  0.924805  0.375000  30.09375       14.007812        6.320312

      Shell Weight  Sex\_F  Sex\_I  Sex\_M  Age
3483     13.070312  False  False   True   12
993      14.320312   True  False  False   13
1427      5.046875  False  False   True   11
3829      9.781250  False  False   True   10
1468      8.390625  False  False   True    9
<class 'pandas.core.frame.DataFrame'>
Index: 3031 entries, 3483 to 658
Data columns (total 11 columns):
 \#   Column          Non-Null Count  Dtype
---  ------          --------------  -----
 0   Length          3031 non-null   float16
 1   Diameter        3031 non-null   float16
 2   Height          3031 non-null   float16
 3   Weight          3031 non-null   float16
 4   Shucked Weight  3031 non-null   float16
 5   Viscera Weight  3031 non-null   float16
 6   Shell Weight    3031 non-null   float16
 7   Sex\_F           3031 non-null   bool
 8   Sex\_I           3031 non-null   bool
 9   Sex\_M           3031 non-null   bool
 10  Age             3031 non-null   int8
dtypes: bool(3), float16(7), int8(1)
memory usage: 77.0 KB
Info:
None
Length distinct values:
[1.725  1.612  1.65   1.362  1.25   1.6875 1.487  1.5625 1.4375 1.45  ]
Diameter distinct values:
[1.3125 1.263  1.15   0.925  1.2    1.162  0.8877 0.8374 1.388  1.0625]
Height distinct values:
[0.5    0.475  0.4    0.375  0.4624 0.425  0.4126 0.4375 0.2876 0.2625]
Weight distinct values:
[50.53 41.1  40.78 25.44 30.1  45.   32.03 32.38 30.19 29.34]
Shucked Weight distinct values:
[25.98  17.03  19.2    9.664 14.01  19.66  16.16  16.42  14.13  11.37 ]
Viscera Weight distinct values:
[9.43  7.273 8.08  4.69  6.32  9.52  7.242 6.082 5.29  2.623]
Shell Weight distinct values:
[13.07  14.32   5.047  9.78   8.39  11.195  7.51   8.22   7.98  10.914]
Sex\_F distinct values:
[False  True]
Sex\_I distinct values:
[False  True]
Sex\_M distinct values:
[ True False]
Age distinct values:
[12 13 11 10  9  8 17  6 19  7]
X\_train shape: (3031, 10)
X\_test shape: (759, 10)
CPU times: total: 0 ns
Wall time: 10.5 ms
    \end{Verbatim}

    \subsection{Rebuild our Model}\label{rebuild-our-model}

\subsubsection{Neural Network
Architecture}\label{neural-network-architecture}

We'll use a simple feedforward neural network with one hidden layer.

\begin{itemize}
\tightlist
\item
  \textbf{Input Layer}

  \begin{itemize}
  \tightlist
  \item
    All features, for now.
  \end{itemize}
\item
  \textbf{Normalizer Layer}

  \begin{itemize}
  \tightlist
  \item
    Adapted to all features in the training data.
  \end{itemize}
\item
  \textbf{Hidden Layers}

  \begin{itemize}
  \tightlist
  \item
    One dense layer with 8 units and ReLU activation.
  \end{itemize}
\item
  \textbf{Output Layer}

  \begin{itemize}
  \tightlist
  \item
    Layer with one output, the predicted age.
  \end{itemize}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{108}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} initialize the all\PYZus{}models dictionary and add the full feature model}
\PY{n}{all\PYZus{}models} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
        \PY{n}{generate\PYZus{}neural\PYZus{}network}\PY{p}{(}
            \PY{n}{X\PYZus{}train}\PY{p}{,}
            \PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{o}{=}\PY{n}{NUM\PYZus{}HIDDEN\PYZus{}LAYERS}\PY{p}{,}
            \PY{n}{num\PYZus{}units}\PY{o}{=}\PY{n}{NUM\PYZus{}UNITS}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{Model: "sequential\_101"}

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃\textbf{ }\textbf{Layer (type)                   }\textbf{ }┃\textbf{ }\textbf{Output Shape          }\textbf{ }┃\textbf{ }\textbf{      Param \#}\textbf{ }┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ normalization\_101               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{10})             │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} │
│ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Normalization})                 │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_202 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{8})              │            \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{88} │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense\_203 (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,135,255}}{Dense})               │ (\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,215,255}}{None}, \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{1})              │             \def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{9} │
└─────────────────────────────────┴────────────────────────┴───────────────┘

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Total params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{118} (476.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{97} (388.00 B)

    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{ Non-trainable params: }\def\tcRGB{\textcolor[RGB]}\expandafter\tcRGB\expandafter{\detokenize{0,175,0}}{21} (88.00 B)

    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 23.8 ms
    \end{Verbatim}

    \subsubsection{Configure the Neural Network
Model}\label{configure-the-neural-network-model}

\begin{itemize}
\tightlist
\item
  \textbf{Optimizer}

  \begin{itemize}
  \tightlist
  \item
    Nadam: Adaptive Moment Estimation with Nesterov Accelerated Gradient

    \begin{itemize}
    \tightlist
    \item
      Combines Adam and Nesterov momentum.
    \item
      Adam is a popular optimizer for its speed and performance.
    \item
      Nesterov momentum helps the optimizer converge faster.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Loss Function}

  \begin{itemize}
  \tightlist
  \item
    Mean Squared Error (MSE)

    \begin{itemize}
    \tightlist
    \item
      This penalizes larger errors more than smaller errors.
    \item
      We took out outliers in the data cleaning step, so this should
      perform better.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Callbacks}

  \begin{itemize}
  \tightlist
  \item
    Model Checkpoint
  \item
    Learning Rate Scheduler

    \begin{itemize}
    \tightlist
    \item
      Start = 0.01 (best from static tuning)
    \item
      Factor = 0.75 (reduce learning rate by 25\%)
    \item
      Patience = 9 epochs (wait 9 epochs of non-improvement before
      reducing learning rate)
    \end{itemize}
  \end{itemize}
\end{itemize}

\subparagraph{Define Common
Configurations}\label{define-common-configurations}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{109}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}

\PY{k}{def} \PY{n+nf}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}
        \PY{n}{optimizer}\PY{p}{:}\PY{n}{keras}\PY{o}{.}\PY{n}{Optimizer}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,}
        \PY{n}{loss\PYZus{}metric}\PY{p}{:}\PY{n+nb}{str}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Return a dictionary of common compile options.}

\PY{l+s+sd}{    :param optimizer: The optimizer to use. Defaults to Adam with LR=0.001.}
\PY{l+s+sd}{    :param loss\PYZus{}metric: The loss metric to use. Defaults to \PYZsq{}mean\PYZus{}squared\PYZus{}error\PYZsq{}.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{return} \PY{p}{\PYZob{}}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{optimizer} \PY{k}{if} \PY{n}{optimizer} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None} \PY{k}{else} \PY{n}{keras}\PY{o}{.}\PY{n}{optimizers}\PY{o}{.}\PY{n}{Nadam}\PY{p}{(}\PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{,}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{loss\PYZus{}metric}
    \PY{p}{\PYZcb{}}


\PY{c+c1}{\PYZsh{} define common callbacks}
\PY{n}{common\PYZus{}learning\PYZus{}rate\PYZus{}scheduler} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ReduceLROnPlateau}\PY{p}{(}
    \PY{n}{monitor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{n}{factor}\PY{o}{=}\PY{l+m+mf}{0.75}\PY{p}{,}
    \PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{9}\PY{p}{,}
    \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{n}{common\PYZus{}checkpoint\PYZus{}options} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{monitor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val\PYZus{}loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{save\PYZus{}best\PYZus{}only}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{k+kc}{True}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{save\PYZus{}weights\PYZus{}only}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{k+kc}{True}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mode}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min}\PY{l+s+s1}{\PYZsq{}}
\PY{p}{\PYZcb{}}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}

\PY{n}{full\PYZus{}feature\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
    \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{NUM\PYZus{}UNITS}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZus{}full\PYZus{}feature.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
    \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 1.51 ms
    \end{Verbatim}

    \subsubsection{Train the Model with Full
Features}\label{train-the-model-with-full-features}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{110}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{common\PYZus{}fit\PYZus{}options} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{X\PYZus{}train}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{y\PYZus{}train}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{NUM\PYZus{}EPOCHS}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{verbose}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{VALIDATION\PYZus{}SPLIT}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} train the model}
\PY{n}{all\PYZus{}histories} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}fit\PYZus{}options}\PY{p}{,}
        \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{common\PYZus{}learning\PYZus{}rate\PYZus{}scheduler}\PY{p}{,} \PY{n}{full\PYZus{}feature\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{NUM\PYZus{}UNITS}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZus{}full\PYZus{}feature.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} plot the training loss}
\PY{n}{plot\PYZus{}training\PYZus{}loss}\PY{p}{(}\PY{n}{all\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

Epoch 50: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 66: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 75: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 84: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.
CPU times: total: 1.22 s
Wall time: 7.96 s
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{features_files/features_14_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Score the Model with Full
Features}\label{score-the-model-with-full-features}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{111}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{full\PYZus{}feature\PYZus{}preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
\PY{n}{full\PYZus{}feature\PYZus{}scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{full\PYZus{}feature\PYZus{}preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Add it to the leaderboard}
\PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{full\PYZus{}feature\PYZus{}scores\PYZus{}df}\PY{p}{)}
\PY{n}{leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 0 ns
Wall time: 84.6 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{111}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
              mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
full\_feature              3.8081             1.422243

              explained\_variance\_score  r2\_score
full\_feature                  0.236575  0.235669
\end{Verbatim}
\end{tcolorbox}
        
    \subsection{Feature Analysis}\label{feature-analysis}

\subsubsection{Distributions of
Features}\label{distributions-of-features}

If you recall from \href{../0-exploratory/exploratory.ipynb}{exploratory
data analysis}, the features are normalized using the training data's
ranges as part of the model's architecture.

Let's take another look at the original distributions of the features.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{112}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} Plotting the distribution of the features}
\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature Distributions}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 125 ms
Wall time: 180 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{112}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Text(0.5, 1.0, 'Feature Distributions')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{features_files/features_18_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Correlation Matrix}\label{correlation-matrix}

To see how the features are correlated with the target label, we'll plot
a heatmap of the correlation matrix.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{113}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Correlation Graph}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Plotting the heatmap to check the correlation between the Target Label and other features}
\PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{p}{)}
\PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{crabs}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{vmin}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{vmax}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GnBu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 46.9 ms
Wall time: 88.7 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{113}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<Axes: title=\{'center': 'Correlation Graph'\}>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{features_files/features_20_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Observations: Correlation
Matrix}\label{observations-correlation-matrix}

\begin{itemize}
\tightlist
\item
  \texttt{Sex\_I} is negatively correlated with \texttt{Age}.

  \begin{itemize}
  \tightlist
  \item
    This makes sense since it's more difficult to determine the sex of
    younger crabs.
  \end{itemize}
\item
  \texttt{Length} and \texttt{Diameter} are highly correlated.

  \begin{itemize}
  \tightlist
  \item
    This makes sense since they are both measurements of the same thing.
  \end{itemize}
\item
  Many of the different weight measurements are correlated.

  \begin{itemize}
  \tightlist
  \item
    Multicollinearity might be an issue here.
  \end{itemize}
\item
  Most size features excluding \texttt{Shucked\ Weight} have similar
  correlations with \texttt{Age}.

  \begin{itemize}
  \tightlist
  \item
    \texttt{Shucked\ Weight} might not be a good feature, since this
    kills the crab.
  \end{itemize}
\end{itemize}

    \subsection{Feature Selection}\label{feature-selection}

Select only the most important features. We'll use \textbf{Recursive
Feature Elimination} (RFE).

In practice, we would want to remove features which, when measured, kill
the crab (e.g.~\texttt{Shucked\ Weight}).

\subsubsection{Recursive Feature Elimination
(RFE)}\label{recursive-feature-elimination-rfe}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train a model for each feature (\(n\) features).
\item
  Plot the validation loss for each model.
\item
  Rank the features by importance (lowest loss -\textgreater{} most
  importance).
\item
  Train a model with the top \(n-k\) features.

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Gradually increase \(k\) until the model's performance degrades
    significantly.
  \end{enumerate}
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{114}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} implement RFE from scratch}

\PY{c+c1}{\PYZsh{} train a model for each feature, in isolation}
\PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} create a model with only the current feature}
    \PY{n}{layer\PYZus{}feature\PYZus{}normalizer} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Normalization}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} normalize the feature}
    \PY{n}{layer\PYZus{}feature\PYZus{}normalizer}\PY{o}{.}\PY{n}{adapt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} build the model}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{n}{feature}\PY{p}{]} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{p}{[}
        \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Input}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{p}{)}\PY{p}{)}\PY{p}{,}     \PY{c+c1}{\PYZsh{} input layer}
        \PY{n}{layer\PYZus{}feature\PYZus{}normalizer}\PY{p}{,}           \PY{c+c1}{\PYZsh{} normalizer}
        \PY{o}{*}\PY{n}{generate\PYZus{}neural\PYZus{}pyramid}\PY{p}{(}\PY{n}{NUM\PYZus{}HIDDEN\PYZus{}LAYERS}\PY{p}{,} \PY{n}{NUM\PYZus{}UNITS}\PY{p}{)}\PY{p}{,}
        \PY{n}{keras}\PY{o}{.}\PY{n}{layers}\PY{o}{.}\PY{n}{Dense}\PY{p}{(}\PY{n}{units}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}         \PY{c+c1}{\PYZsh{} output layer}
    \PY{p}{]}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} compile the model}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} checkpoint}
    \PY{n}{model\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
        \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{NUM\PYZus{}UNITS}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{feature}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} fit options}
    \PY{n}{model\PYZus{}fit\PYZus{}options} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{k}\PY{p}{:} \PY{n}{v} \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{v} \PY{o+ow}{in} \PY{n}{common\PYZus{}fit\PYZus{}options}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)} \PY{k}{if} \PY{n}{k} \PY{o}{!=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}
    \PY{n}{model\PYZus{}fit\PYZus{}options}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{feature}\PY{p}{]}
    \PY{c+c1}{\PYZsh{} train the model}
    \PY{n}{all\PYZus{}histories}\PY{p}{[}\PY{n}{feature}\PY{p}{]} \PY{o}{=} \PYZbs{}
        \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
            \PY{o}{*}\PY{o}{*}\PY{n}{model\PYZus{}fit\PYZus{}options}\PY{p}{,}
            \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{common\PYZus{}learning\PYZus{}rate\PYZus{}scheduler}\PY{p}{,} \PY{n}{model\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} checkpoint the best weights for this iteration}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{NUM\PYZus{}UNITS}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{feature}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} score the model}
    \PY{n}{preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
    \PY{n}{scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{feature}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} add it to the leaderboard}
    \PY{n}{leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{scores\PYZus{}df}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} end for}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

Epoch 22: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 36: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 45: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 55: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.

Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.

Epoch 82: ReduceLROnPlateau reducing learning rate to 0.0013348387728910893.

Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0010011291014961898.

Epoch 100: ReduceLROnPlateau reducing learning rate to 0.0007508468697778881.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step

Epoch 15: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 24: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 33: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 48: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.

Epoch 75: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.

Epoch 84: ReduceLROnPlateau reducing learning rate to 0.0013348387728910893.

Epoch 96: ReduceLROnPlateau reducing learning rate to 0.0010011291014961898.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step

Epoch 16: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 25: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 34: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 43: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.

Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.

Epoch 70: ReduceLROnPlateau reducing learning rate to 0.0013348387728910893.

Epoch 79: ReduceLROnPlateau reducing learning rate to 0.0010011291014961898.

Epoch 88: ReduceLROnPlateau reducing learning rate to 0.0007508468697778881.

Epoch 97: ReduceLROnPlateau reducing learning rate to 0.000563135152333416.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}ahest\textbackslash{}dev\textbackslash{}.venv-ai\textbackslash{}Lib\textbackslash{}site-packages\textbackslash{}numpy\textbackslash{}core\textbackslash{}\_methods.py:152:
RuntimeWarning: overflow encountered in reduce
  arrmean = umr\_sum(arr, axis, dtype, keepdims=True, where=where)
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

Epoch 58: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 67: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 85: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 94: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}ahest\textbackslash{}dev\textbackslash{}.venv-ai\textbackslash{}Lib\textbackslash{}site-packages\textbackslash{}numpy\textbackslash{}core\textbackslash{}\_methods.py:187:
RuntimeWarning: overflow encountered in reduce
  ret = umr\_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

Epoch 62: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 71: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 83: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 92: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step

Epoch 16: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 25: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 41: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 65: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 97: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step

Epoch 20: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 29: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 38: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 47: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 68: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.

Epoch 77: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.

Epoch 86: ReduceLROnPlateau reducing learning rate to 0.0013348387728910893.

Epoch 95: ReduceLROnPlateau reducing learning rate to 0.0010011291014961898.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step

Epoch 19: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 28: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 37: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 46: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.

Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.

Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0013348387728910893.

Epoch 82: ReduceLROnPlateau reducing learning rate to 0.0010011291014961898.

Epoch 91: ReduceLROnPlateau reducing learning rate to 0.0007508468697778881.

Epoch 100: ReduceLROnPlateau reducing learning rate to 0.000563135152333416.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step

Epoch 12: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 21: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 30: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 39: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 48: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.

Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.

Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0013348387728910893.

Epoch 75: ReduceLROnPlateau reducing learning rate to 0.0010011291014961898.

Epoch 84: ReduceLROnPlateau reducing learning rate to 0.0007508468697778881.

Epoch 93: ReduceLROnPlateau reducing learning rate to 0.000563135152333416.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step

Epoch 13: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 23: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 32: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 52: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.

Epoch 70: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.

Epoch 85: ReduceLROnPlateau reducing learning rate to 0.0013348387728910893.

Epoch 94: ReduceLROnPlateau reducing learning rate to 0.0010011291014961898.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
CPU times: total: 17.1 s
Wall time: 1min 18s
    \end{Verbatim}

    \paragraph{Plot the Validation Loss for Each
Feature}\label{plot-the-validation-loss-for-each-feature}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{115}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss\PYZus{}from\PYZus{}dict}\PY{p}{(}\PY{n}{all\PYZus{}histories}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 15.6 ms
Wall time: 76.6 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{features_files/features_25_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Show the Leaderboard}\label{show-the-leaderboard}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{116}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} show the leaderboard}
\PY{n}{leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{116}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
full\_feature              3.808100             1.422243
Length                    5.529498             1.773356
Diameter                  5.321249             1.742332
Height                    5.053789             1.699966
Weight                    7.988140             2.165772
Shucked Weight            7.987278             2.165660
Viscera Weight            5.423214             1.758038
Shell Weight              4.813616             1.660682
Sex\_F                     7.323804             2.079437
Sex\_I                     6.424611             1.893552
Sex\_M                     7.874757             2.157090

                explained\_variance\_score      r2\_score
full\_feature                2.365749e-01  2.356686e-01
Length                     -7.443867e-01 -7.445461e-01
Diameter                   -6.521820e-01 -6.522463e-01
Height                     -5.566758e-01 -5.582471e-01
Weight                     -8.781164e+12 -8.783052e+12
Shucked Weight             -8.781164e+12 -8.782105e+12
Viscera Weight             -7.496330e-01 -7.496433e-01
Shell Weight               -1.861438e-01 -1.862737e-01
Sex\_F                      -1.661453e+01 -1.661489e+01
Sex\_I                      -2.870045e+00 -2.870567e+00
Sex\_M                      -1.927305e+01 -1.927815e+01
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Bar Chart of Feature
Importance}\label{bar-chart-of-feature-importance}

Show the Mean Absolute Error for the model trained with each individual
feature.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{117}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} sort the leaderboard by mean absolute error for easier viewing}
\PY{n}{sorted\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}absolute\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Individual Feature Score (Lower is Better)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{sorted\PYZus{}leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{sorted\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}absolute\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{45}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mf}{1.2}\PY{p}{,} \PY{l+m+mf}{2.3}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 16.6 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{117}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(1.2, 2.3)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{features_files/features_29_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Feature Importance
Observations}\label{feature-importance-observations}

\begin{itemize}
\tightlist
\item
  \texttt{Shucked\ Weight} is the worst feature.

  \begin{itemize}
  \tightlist
  \item
    This kills the crab.

    \begin{itemize}
    \tightlist
    \item
      We should remove it first.
    \end{itemize}
  \end{itemize}
\item
  Surprisingly, \texttt{Weight} is not as important as the other size
  features.

  \begin{itemize}
  \tightlist
  \item
    This might be due to multicollinearity.

    \begin{itemize}
    \tightlist
    \item
      Let's remove it as well.
    \end{itemize}
  \end{itemize}
\item
  \texttt{Shell\ Weight} is the most important feature.

  \begin{itemize}
  \tightlist
  \item
    Older crabs must have thicker shells.
  \end{itemize}
\end{itemize}

    \subsubsection{Eliminate the Worst
Features}\label{eliminate-the-worst-features}

Next, we'll gradually remove the worst features from the full-featured
model and retrain.

Each step, the model will be trained with one less feature until we run
out of features.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{118}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} implement RFE from scratch}
\PY{n}{X\PYZus{}train\PYZus{}rfe} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
\PY{n}{X\PYZus{}test\PYZus{}rfe} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
\PY{n}{rfe\PYZus{}histories} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{all\PYZus{}histories}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{\PYZcb{}}
\PY{n}{rfe\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
\PY{c+c1}{\PYZsh{} starting with the worst feature, gradually cut features and retrain}
\PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{sorted\PYZus{}leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{index}\PY{p}{:}
    \PY{k}{if} \PY{n}{feature} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{full\PYZus{}feature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} don\PYZsq{}t remove \PYZsq{}full\PYZus{}feature\PYZsq{} as it\PYZsq{}s an aggregate}
        \PY{k}{continue}
    \PY{c+c1}{\PYZsh{} drop the next worst feature}
    \PY{n}{X\PYZus{}train\PYZus{}rfe} \PY{o}{=} \PY{n}{X\PYZus{}train\PYZus{}rfe}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{X\PYZus{}test\PYZus{}rfe} \PY{o}{=} \PY{n}{X\PYZus{}test\PYZus{}rfe}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{n}{feature}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Removed feature: }\PY{l+s+si}{\PYZob{}}\PY{n}{feature}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}train\PYZus{}rfe shape: }\PY{l+s+si}{\PYZob{}}\PY{n}{X\PYZus{}train\PYZus{}rfe}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{k}{if} \PY{n}{X\PYZus{}train\PYZus{}rfe}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} no more features to remove}
        \PY{k}{break}
    \PY{c+c1}{\PYZsh{} create a new model with the reduced feature set}
    \PY{c+c1}{\PYZsh{} model\PYZus{}name = f\PYZsq{}\PYZob{}\PYZdq{}\PYZus{}\PYZdq{}.join([col[:min(5, len(col))] for col in X\PYZus{}train\PYZus{}rfe.columns])\PYZcb{}\PYZsq{}}
    \PY{n}{model\PYZus{}name} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Without\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{feature}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{n}{model\PYZus{}name}\PY{p}{]} \PY{o}{=} \PYZbs{}
        \PY{n}{generate\PYZus{}neural\PYZus{}network}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}rfe}\PY{p}{,} \PY{n}{num\PYZus{}hidden\PYZus{}layers}\PY{o}{=}\PY{n}{NUM\PYZus{}HIDDEN\PYZus{}LAYERS}\PY{p}{,} \PY{n}{num\PYZus{}units}\PY{o}{=}\PY{n}{NUM\PYZus{}UNITS}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} compile the model}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{n}{model\PYZus{}name}\PY{p}{]}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}compile\PYZus{}options}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} checkpoint}
    \PY{n}{model\PYZus{}checkpoint} \PY{o}{=} \PY{n}{keras}\PY{o}{.}\PY{n}{callbacks}\PY{o}{.}\PY{n}{ModelCheckpoint}\PY{p}{(}
        \PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{NUM\PYZus{}UNITS}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{model\PYZus{}name}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
        \PY{o}{*}\PY{o}{*}\PY{n}{common\PYZus{}checkpoint\PYZus{}options}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} fit options}
    \PY{n}{model\PYZus{}fit\PYZus{}options} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{k}\PY{p}{:} \PY{n}{v} \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{v} \PY{o+ow}{in} \PY{n}{common\PYZus{}fit\PYZus{}options}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)} \PY{k}{if} \PY{n}{k} \PY{o}{!=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}
    \PY{n}{model\PYZus{}fit\PYZus{}options}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{X\PYZus{}train\PYZus{}rfe}
    \PY{c+c1}{\PYZsh{} train the model}
    \PY{n}{rfe\PYZus{}histories}\PY{p}{[}\PY{n}{model\PYZus{}name}\PY{p}{]} \PY{o}{=} \PYZbs{}
        \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{n}{model\PYZus{}name}\PY{p}{]}\PY{o}{.}\PY{n}{fit}\PY{p}{(}
            \PY{o}{*}\PY{o}{*}\PY{n}{model\PYZus{}fit\PYZus{}options}\PY{p}{,}
            \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{common\PYZus{}learning\PYZus{}rate\PYZus{}scheduler}\PY{p}{,} \PY{n}{model\PYZus{}checkpoint}\PY{p}{]}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} checkpoint the best weights for this iteration}
    \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{n}{model\PYZus{}name}\PY{p}{]}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{n}{MODEL\PYZus{}CHECKPOINT\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{NUM\PYZus{}UNITS}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{model\PYZus{}name}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{.weights.h5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} score the model}
    \PY{n}{preds} \PY{o}{=} \PY{n}{all\PYZus{}models}\PY{p}{[}\PY{n}{model\PYZus{}name}\PY{p}{]}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}rfe}\PY{p}{)}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}
    \PY{n}{scores\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}model}\PY{p}{(}\PY{n}{preds}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{model\PYZus{}name}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} add it to the leaderboard}
    \PY{n}{rfe\PYZus{}leaderboard\PYZus{}df} \PY{o}{=} \PY{n}{score\PYZus{}combine}\PY{p}{(}\PY{n}{rfe\PYZus{}leaderboard\PYZus{}df}\PY{p}{,} \PY{n}{scores\PYZus{}df}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} end for}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Removed feature: Weight
X\_train\_rfe shape: (3031, 9)

Epoch 59: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
Removed feature: Shucked Weight
X\_train\_rfe shape: (3031, 8)

Epoch 51: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 60: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 69: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 78: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 94: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
Removed feature: Sex\_M
X\_train\_rfe shape: (3031, 7)

Epoch 50: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 69: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 78: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 87: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
Removed feature: Sex\_F
X\_train\_rfe shape: (3031, 6)

Epoch 33: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 45: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 76: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 85: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
Removed feature: Sex\_I
X\_train\_rfe shape: (3031, 5)

Epoch 55: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 67: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 76: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 85: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 94: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
Removed feature: Length
X\_train\_rfe shape: (3031, 4)

Epoch 34: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 43: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 52: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 61: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 70: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.

Epoch 79: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.

Epoch 88: ReduceLROnPlateau reducing learning rate to 0.0013348387728910893.

Epoch 97: ReduceLROnPlateau reducing learning rate to 0.0010011291014961898.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
Removed feature: Viscera Weight
X\_train\_rfe shape: (3031, 3)

Epoch 33: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 42: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 51: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 76: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 85: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.

Epoch 94: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
Removed feature: Diameter
X\_train\_rfe shape: (3031, 2)

Epoch 17: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 39: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 49: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 58: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 67: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.

Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.

Epoch 85: ReduceLROnPlateau reducing learning rate to 0.0013348387728910893.

Epoch 94: ReduceLROnPlateau reducing learning rate to 0.0010011291014961898.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
Removed feature: Height
X\_train\_rfe shape: (3031, 1)

Epoch 16: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.

Epoch 31: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.

Epoch 40: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.

Epoch 62: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.

Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.

Epoch 80: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.

Epoch 89: ReduceLROnPlateau reducing learning rate to 0.0013348387728910893.

Epoch 98: ReduceLROnPlateau reducing learning rate to 0.0010011291014961898.
\textbf{24/24} \textcolor{ansi-green}{━━━━━━━━━━━━━━━━━━━━} \textbf{0s} 1ms/step
Removed feature: Shell Weight
X\_train\_rfe shape: (3031, 0)
CPU times: total: 15.3 s
Wall time: 1min 11s
    \end{Verbatim}

    \paragraph{Plot the Validation Loss for Each
Model}\label{plot-the-validation-loss-for-each-model}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{119}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plot\PYZus{}training\PYZus{}loss\PYZus{}from\PYZus{}dict}\PY{p}{(}\PY{n}{rfe\PYZus{}histories}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 31.2 ms
Wall time: 62.7 ms
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{features_files/features_34_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{Show the Leaderboard}\label{show-the-leaderboard}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{120}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} show the leaderboard}
\PY{n}{rfe\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{120}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                        mean\_squared\_error  mean\_absolute\_error  \textbackslash{}
full\_feature                      3.808100             1.422243
Without\_Weight                    3.949613             1.463730
Without\_Shucked Weight            4.300336             1.527793
Without\_Sex\_M                     4.454281             1.566167
Without\_Sex\_F                     4.236380             1.502735
Without\_Sex\_I                     4.330109             1.520248
Without\_Length                    4.576464             1.597767
Without\_Viscera Weight            4.535533             1.597340
Without\_Diameter                  4.783702             1.665123
Without\_Height                    5.123232             1.708974

                        explained\_variance\_score  r2\_score
full\_feature                            0.236575  0.235669
Without\_Weight                          0.141759  0.140100
Without\_Shucked Weight                  0.034514  0.034506
Without\_Sex\_M                           0.102224  0.101621
Without\_Sex\_F                           0.048044  0.047874
Without\_Sex\_I                          -0.031750 -0.033034
Without\_Length                         -0.026461 -0.027057
Without\_Viscera Weight                 -0.148357 -0.148518
Without\_Diameter                       -0.214642 -0.214778
Without\_Height                         -0.341322 -0.341374
\end{Verbatim}
\end{tcolorbox}
        
    \paragraph{Bar Chart of Feature
Unimportance}\label{bar-chart-of-feature-unimportance}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{121}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature Unimportance (Lower is Better)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{bar}\PY{p}{(}\PY{n}{rfe\PYZus{}leaderboard\PYZus{}df}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{rfe\PYZus{}leaderboard\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}absolute\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{45}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Feature(s) Removed (Accumulative)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MAE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mf}{1.2}\PY{p}{,} \PY{l+m+mf}{1.8}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 16 ms
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{121}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(1.2, 1.8)
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{features_files/features_38_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Feature Selection
Observations}\label{feature-selection-observations}

\begin{itemize}
\tightlist
\item
  Removing \texttt{Weight} improved the model, if it was removed first.

  \begin{itemize}
  \tightlist
  \item
    But it degraded the model if it was removed \emph{after}
    \texttt{Shucked\ Weight}.
  \item
    This suggests multicollinearity was an issue.

    \begin{itemize}
    \tightlist
    \item
      We should remove this \texttt{Shucked\ Weight} feature. - This
      kills the crab.
    \item
      Remove \texttt{Viscera\ Weight} as well. - This is likely a linear
      combination.
    \end{itemize}
  \end{itemize}
\item
  Removing \texttt{Sex\_F} improved upon the previous iteration.

  \begin{itemize}
  \tightlist
  \item
    But it likely was not able to provide insight without its
    counterparts.

    \begin{itemize}
    \tightlist
    \item
      We should keep all or none of the one-hot encoded features in this
      case.
    \end{itemize}
  \end{itemize}
\end{itemize}

    \subsubsection{Feature Importance with
SHAP}\label{feature-importance-with-shap}

We can use SHAP to \href{https://stackoverflow.com/a/69523421}{determine
the feature importance} of the model, if we enable widgets.

\emph{\textbf{Note}: This is not enabled in this notebook.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{122}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} import shap}

\PY{c+c1}{\PYZsh{} load JS visualization code to notebook}
\PY{c+c1}{\PYZsh{} shap.initjs()}

\PY{c+c1}{\PYZsh{} explain the model\PYZsq{}s predictions using SHAP}
\PY{c+c1}{\PYZsh{} explainer = shap.Explainer(all\PYZus{}models[\PYZsq{}full\PYZus{}feature\PYZsq{}], X\PYZus{}train)}
\PY{c+c1}{\PYZsh{} shap\PYZus{}values = explainer(X\PYZus{}test)}

\PY{c+c1}{\PYZsh{} visualize the first prediction\PYZsq{}s explanation}
\PY{c+c1}{\PYZsh{} shap.plots.waterfall(shap\PYZus{}values[0])}


\PY{c+c1}{\PYZsh{} feature\PYZus{}names = X\PYZus{}train.columns}


\PY{c+c1}{\PYZsh{} rf\PYZus{}resultX = pd.DataFrame(shap\PYZus{}values, columns = feature\PYZus{}names)}
\PY{c+c1}{\PYZsh{} }
\PY{c+c1}{\PYZsh{} vals = np.abs(rf\PYZus{}resultX.values).mean(0)}
\PY{c+c1}{\PYZsh{} }
\PY{c+c1}{\PYZsh{} shap\PYZus{}importance = pd.DataFrame(list(zip(feature\PYZus{}names, vals)),}
\PY{c+c1}{\PYZsh{}                                   columns=[\PYZsq{}col\PYZus{}name\PYZsq{},\PYZsq{}feature\PYZus{}importance\PYZus{}vals\PYZsq{}])}
\PY{c+c1}{\PYZsh{} shap\PYZus{}importance.sort\PYZus{}values(by=[\PYZsq{}feature\PYZus{}importance\PYZus{}vals\PYZsq{}],}
\PY{c+c1}{\PYZsh{}                                ascending=False, inplace=True)}
\PY{c+c1}{\PYZsh{} shap\PYZus{}importance.head()}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

    \subsubsection{Note on Python Packages for Feature
Importance}\label{note-on-python-packages-for-feature-importance}

Be careful when using feature importance packages. They can introduce
vulnerabilities to your system.

\paragraph{lime==0.2.0.1 - Security Health
(61/100)}\label{lime0.2.0.1---security-health-61100}

\begin{quote}
https://snyk.io/advisor/python/lime
\end{quote}

LIME is a popular library for feature importance in Python, but it has
not been maintained for 4 years as of this writing.

Perhaps it's time for a new library to take its place.

\paragraph{shapley==1.0.3 - Security Health
(39)}\label{shapley1.0.3---security-health-39}

\begin{quote}
https://snyk.io/advisor/python/shapley
\end{quote}

Be cautious when using SHAPLEY, as it has a very low security health
score.

It's important to keep your dependencies up-to-date and secure.

\paragraph{shap==0.45.0 - Security Health
(94)}\label{shap0.45.0---security-health-94}

\begin{quote}
https://snyk.io/advisor/python/shap

A unified approach to explain the output of any machine learning model.
\end{quote}

\paragraph{keras==3.3.3 - Security Health
(97)}\label{keras3.3.3---security-health-97}

\begin{quote}
https://snyk.io/advisor/python/keras
\end{quote}

\emph{whew}

    \subsection{Feature Engineering}\label{feature-engineering}

Feature engineering is a crucial part of the machine learning process.
It can make or break a model.

You can:

\begin{itemize}
\tightlist
\item
  Reduce features (with RFE, for example).
\item
  Combine features (avoid collinearity).
\item
  Synthesize new features.
\end{itemize}

We'll skip combining and synthesizing today, but you can try it on your
own.

Feature engineering is an art, not a science. It requires domain
knowledge and creativity.

Bias can easily be introduced by feature engineering, so be careful.
However, with careful consideration you can reduce your model's bias.

I don't know a much about selling crabs, so I'll leave this to the
curious reader.

\subsubsection{Feature Reduction}\label{feature-reduction}

To save crabs, let's remove the features, \texttt{Shucked\ Weight},
\texttt{Viscera\ Weight}, and \texttt{Shell\ Weight}.

We'll re-train our model in the next section.

\paragraph{Remove the Shucked Weight
Feature}\label{remove-the-shucked-weight-feature}

This feature likely caused multicollinearity issues. It also kills the
crab. See ya!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{123}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shucked Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Viscera Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shell Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shucked Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Viscera Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shell Weight}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 0 ns
    \end{Verbatim}

    \subsection{Save the Data}\label{save-the-data}

So we can pick this back up on the
\href{../3-evaluation/evaluation.ipynb}{next \ldots and final\ldots{}
step}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{124}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}time}
\PY{c+c1}{\PYZsh{} save the training and test data separately}
\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{join}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{outer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{to\PYZus{}feather}\PY{p}{(}\PY{n}{NEXT\PYZus{}CACHE\PYZus{}FILE}\PY{p}{)}
\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{join}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{outer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{to\PYZus{}feather}\PY{p}{(}\PY{n}{NEXT\PYZus{}CACHE\PYZus{}FILE}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.feather}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZus{}test.feather}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: total: 0 ns
Wall time: 4.01 ms
    \end{Verbatim}

    \subsection{Onwards to Final
Evaluation}\label{onwards-to-final-evaluation}

See the \href{../3-evaluation/evaluation.ipynb}{next section} for the
final evaluation.

\href{https://nbviewer.org/github/ahester57/ai_workshop/blob/master/notebooks/time_for_crab/3-evaluation/evaluation.ipynb}{\texttt{\textless{}html\ link\textgreater{}}}
for model evaluation.
\href{../3-evaluation/evaluation.html}{\texttt{\textless{}localhost\ html\ link\textgreater{}}}
for model evaluation.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}

# Model Evaluation :microscope: :chart_with_upwards_trend:

*In which we hope all the work we put into our model pays off.*

## Table of Contents

- [Evaluation](evaluation.ipynb) :microscope: :chart_with_upwards_trend:

See the [notebook](https://nbviewer.org/github/ahester57/ai_workshop/blob/master/notebooks/time_for_crab/3-evaluation/evaluation.ipynb) for more.

## Evaluation Metrics

We'll use some of the following metrics to evaluate our model:

- **Mean Absolute Error (MAE)**: The average of the absolute differences between predictions and actual values.
- **Mean Squared Error (MSE)**: The average of the squared differences between predictions and actual values.
- **Root Mean Squared Error (RMSE)**: The square root of the average of the squared differences between predictions and actual values.
- **R² Score (R²)**: The proportion of the variance in the dependent variable that is predictable from the independent variables.
- **Adjusted R² Score (Adjusted R²)**: The R² score adjusted for the number of predictors in the model.
- **Mean Absolute Percentage Error (MAPE)**: The average of the absolute percentage differences between predictions and actual values.
- **Mean Percentage Error (MPE)**: The average of the percentage differences between predictions and actual values.
- **Mean Absolute Scaled Error (MASE)**: The mean absolute error of the model divided by the mean absolute error of a naive model.
- **Mean Squared Logarithmic Error (MSLE)**: The average of the squared logarithmic differences between predictions and actual values.
- **Root Mean Squared Logarithmic Error (RMSLE)**: The square root of the average of the squared logarithmic differences between predictions and actual values.
- **Receiver Operating Characteristic (ROC) Curve**: A graphical representation of the true positive rate against the false positive rate.
- **Regression Receiver Operating Characteristic (RROC) Curve**: A graphical representation of the true positive rate against the false positive rate for regression problems.
- **Precision-Recall Curve**: A graphical representation of the precision against the recall.
- **F1 Score**: The harmonic mean of precision and recall.
- **Confusion Matrix**: A table that shows the number of true positives, false positives, true negatives, and false negatives.
- **Classification Report**: A text report showing the main classification metrics.
- **ROC AUC Score**: The area under the receiver operating characteristic curve.
- **PR AUC Score**: The area under the precision-recall curve.
- **Cohen's Kappa Score**: A statistic that measures inter-rater agreement for qualitative items.
- **Matthews Correlation Coefficient (MCC)**: A measure of the quality of binary classifications.
- **Jaccard Score**: The size of the intersection divided by the size of the union of two label sets.
- **Hamming Loss**: The fraction of labels that are incorrectly predicted.
- **Zero-One Loss**: The fraction of misclassifications.
- **Log Loss**: The negative log-likelihood of the true labels given the predicted probabilities.
- **Brier Score**: The mean squared difference between predicted probabilities and the true labels.
- **Explained Variance Score**: The proportion of the variance in the dependent variable that is predictable from the independent variables.
- **Max Error**: The maximum residual error.

---

Return to [Time for Crab](../README.md).
